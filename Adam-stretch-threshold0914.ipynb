{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "# from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, Input.\n",
    "\n",
    "# from tensorflow.keras.layers import Embedding\n",
    "# from keras import models\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers # L2규제\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # data augmentation\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler # callbacks 설정\n",
    "\n",
    "#from tqdm.notebook import tqdm # 모델학습 진행 시간 파악\n",
    "import random # random seed를 뽑을때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "submission = pd.read_csv('data/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 크기 확대\n",
    "x_train = train.drop(['id', 'digit', 'letter'], axis=1).values\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') #resize에러 float32로 바꿔 해결함\n",
    "x_train = [cv2.resize(im,dsize=(84,84),interpolation=cv2.INTER_LINEAR) for im in x_train]\n",
    "#x_train = x_train/255 # data 정규화\n",
    "#img=x_train[0].reshape(28,28).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20a85980f08>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAnG0lEQVR4nO2da4yk2Vnff885533funT3zPTser14HWyiFeAgYZMVmDiKHAzIEAvzJQhLRAglcj6QBAgS2ImUiA+R/CFCICVCWnEJBMJFBgKyEGCZoChStPFyCbe1sTEbPN71zs5M36vqvZzz5MP79m2me7p6uqe7quv5SaXqOnV5z1H1v845zznn+YuqYhjG1cdddgUMw7gYTOyGsSCY2A1jQTCxG8aCYGI3jAXBxG4YC8KZxC4i7xWRT4vIZ0XkQ+dVKcMwzh951HV2EfHAXwLfBNwCPgl8QFX/4vyqZxjGeRHO8N6vBT6rqp8DEJFfAt4PHCv2XArtMTzDJQ3DeBgTdqi0lKOeO4vY3wR8/sDjW8DXPewNPYZ8nbznDJc0DONhvKCfOPa5s4j9qF+PB+YEIvJB4IMAPQZnuJxhGGfhLAG6W8CbDzx+Bnjl/hep6vOq+pyqPpdRnOFyhmGchbOI/ZPAsyLyVhHJge8EfvN8qmUYxnnzyMN4VW1E5F8AvwN44KdV9c/PrWaGYZwrZ5mzo6q/BfzWOdXFMIzHiO2gM4wFwcRuGAuCid0wFgQTu2EsCCZ2w1gQTOyGsSCY2A1jQTCxG8aCcKZNNcbVRkLALQ0hnOLfpGmIm9uQ4uOrmPFImNiNY5EQkBvX0V4+/XvGJW5SkiYm9lnDxG48iPOI90ieo96Bc6gXcPfN+lSRJoEqSHfi2Tkkz5GY0Bith58hTOzGYUTw11aQlaVO5B6ANMiplzJw+2kM/KghbE6QA3rWLCA3b+BTQje3ievr7Y+BcemY2I0HkF6BDvv7vTWQgqMZevSA2ElK2BIO5SwJHg2+7fXL6uIqbZyIiX1RcB7X7yHZMV95CEivB8GTVgakQTuEj4VHg1Be85TX22G8q8BFJYWAhiUk7YtdGsWXEYkJ5xyhV0AT0ckEmmb/eklJ4wla2w/CRWFiXxAkC7gnVtHi6GBbGvaobvZImQMHKkIshMkNR8yFegWqa4okyDcEPxZEYfSkP/Q5vlR66wlfKnKzgLSMqxP53QluZ3Lgggl35x5x3cR+UZjYrzoiIA4JAc0ztJcd8bwQhxnN0JOCgIAKxEJoBkLsQTNQmkErdlfKg9kGFUQBJzQTQR2Itq/zlZB6AYn715YmQZaD86DJ5vUXgIn9iuOXl5HhAPo96ieWSb39njh5oVnyxNwRs1bc6qHptwKPOZSrCS0UlmuWV8bE5NhZ6SPjAz26gh+5dnjfCNWyw0XwEwgjxdWAFmQHri1JyUQIeYaWJWl9Az04zDfOnRPFLiI/DbwPuK2qX9WVrQK/DLwFeBn4DlVde3zVNB4JEWTQR2+skPoZ9bWc2DsYdBPGq45msF+mDqprUC+3Inc3SnpFzY3hmL+1vEajjpeLVbbH+8lDUxLKjR5u2+/10KKQbTlSEHwFvnbogc05EkHqHgGQUQZb24fn9Ma5M03P/l+A/wT83IGyDwGfUNWPdLZPHwJ++PyrZzwSzuPyDLIMBn3SICf2Ak3fEQshhVboybM3TFcHKYB6qK8l0nKDKyI3VkYsFSVPDzZ5duk2dfI0yXEvG1InR9UEYhJi42mcQhKIAglcLbiyHS1Uy0LMPa5WsrEiUQ9F+0XkwTzkxrlyothV9X+KyFvuK34/8O7u758Ffh8T+8zg+r02GJdn1E8sU13PiT3HeNURe0IzhHpJUQcaFAViX9HlBpdFVlbGvGFpm+V8wt9ZeZUbYYdniy/ytuwOE3X86fBpblU3uVMv8fLoJpMYqFYCTXJsVwX3tgfUVaD2GeoCJKiuAwmKNcfyrYSrTdoXzaPO2Z9S1VcBVPVVEXnDOdbJOCvOtcG4IicVnlQIMRdSDimHWEAzbMW+ixaJfFiRZZHV4YinBptcz8a8KV/jpt/mzWGdZ0KfUms20j08ipPE69UywUWgBCD3kZ0yR1VoskDKFRKk7jphJOiR5kTG4+axB+jMEWaGEUi9BFlCsoTI4d62TIEvVDdYc60/X8YXKdXzqeppXqlu8Ep5nc/vXKeM+/9GO1XO1nafWHqoXBulT+BLwdVCGHdRe+PCeVSxvyYiT3e9+tPA7eNeqKrPA88DrMiqfc0zhDpFBg1Fv35A6ABVCtwa3wCgVr93/5fjN/JaucJrk2Ve21qmjvtR9nKSETcyXCd0AElC2JG96PzBTTjGxfGo59l/E/ju7u/vBn7jfKpjPDakC8plbRBur1h0T+j3Cz6qUKsjIUR1RIRKPRHXlqsQVUhJaBpHVXli45DUBugktkE6iez18NarXx7TLL39Im0w7gkRuQX8e+AjwK+IyD8F/gb4x4+zksbZSQHq5S4w53lg3nxUz+6nVKaqUI1ydOLbKHzTfrgv2x5dEu1au3GpTBON/8AxT5n38hyhTohFG3U/K+m+XwpV0Nrhxu7Qzjpp2qG79eazge2gMwDwPjHIa7xLDLMH96tvx4Jb1U1GKefVyTXuTIbcGw8YjQpS5XE7nrBz4EdAwZcm9FnCxG4A0MsavmRpg55vcEco9F41ZL0eMI4ZL2+usjkpWqGvFUgt5OuOsH34PZIe+BjjEjGxGw+QVEjqOWrz6iQGxnWgqgKp8kgtuEpwFfhpDrB1gTtJisQETYSUUDsI89gxsRsAlHXg1Z2VI3v1g1RNYH1jSBoF3MiTr7cHYPINyLf0wdNw95HvJHr3KlwZCbc30bUNUtOgte2Lf9yY2A0AmujYHPVOfF1de9J2hhs7wkgI222kPd9S8q2Tx+3ZVkNYnyCTGt3YJK7Z+amLwsS+IEhSXCX4SXswJQUFFVLlqY7ZbSGihCzi3AERqyB1+zl+LIQJuEoJEyWMTz6XHkYNMiqRqiZVth53kZjYFwTXQL7VroE3A9Clds86O5409ke+R7OErCiS7W+8SSqEHUexJoQd6K0lfKUU9xqyjcn+Jvjj6rE1Ql+7Q6oqG7pfMCb2RUHB1Yr37YGYXSQKHJftWdzemrru3idB6i4PXa34SvGl4icNbnxyTy2TijQeW6KKS8DEbhxPhLidMT7oCFO5vR1yeyj4UQ1310/8SB1P2nzyxoVjYjeORaIgOx4OaFtqeWDrq6giOxPi7ddP/lBbYrs0TOxXkZSQugERXBVxlcc7xdVtcM5V4Eft3+o4JObjUK9dEskjnhPZc4IhKdrUJuoZxMR+BUnjCdy+g4RASAlXLxF7AchpCiHfaFNTaejSRRUnfiTNAOLgGAELpJU+/umnYFIS765ZPvgZxMR+FUmRNBqBCGHQb/PRpUQYhfb46e7LgqDiDpUdiUDKhPiQHTMp97hhHxFBvENtVW3mMLFfcXQ0RrzHVQVZEXDV/leuoT1+GvO2p28KOTSkV7+fXnrq6wWPXFvB9wp0UpImk5PfZFwIJvarjCpxawu2d3DDAUEE3z+47iYUt6V1fxlmlE/kJL+v9lgIE386sZMFuL7Ser2tbUJZ2vx9RjCxX3VUQSPUNVLVD9ou03XmDsIo7DvCuNbVxUVFY5ttRroU0Q/d/y4Cu8aO/lETIRmPAxP7gpCqGu6tI+Hor9zdC/Tv9MB70nKfOMgJ/QCS0RTdMly3scayzswn06SlejOtQcQbaX/Xn1fVHzdXmDkjRdLW1smvc57w1JPIjRVcnRMLh6sd6lp3l0fOWmhcOtN8dQ3wg6r6lcA7ge8Vkbex7wrzLPCJ7rExrziPGwzwS0N02CcNC2I/I/YdTb9NVHkc6iDljjTISb3QrrsbM8c0OeheBXYNIbZE5CXgTZgrzJXC9Xu4J2+ieUbzxNJhF5ndiPxxGhaoh4Gm7/FlIrs3QUpLUzNrnGrO3tlAvQN4gSldYcwkYj4Q79GstXVOuSflQsz2nWSgzUh7XG6LlAnQLuXZUH82mfprEZEl4FeB71fVzWnfp6rPq+pzqvpcxhRbtYyZJBZtKupqmYcO6Y3ZZSqxi0hGK/RfUNVf64pf69xgOMkVxph/UtbaODdDJZ1m3d2YGU4Uu4gI8FPAS6r6oweeMleYK4CEgBQF5NkhC+X70XaUvn879JwF5OaBaebs7wL+CfCnIvLHXdm/wVxh5h/ncTduIMM+GjzYJpgrzTTR+P/F8XFYc4WZY8QJ0ivQQZdo8hF7aFG13n0OsJ9yo+UMYjWhzwcmduPMiB10mQtM7MaZsZ59PjCxG8aCYGI3jAXBjrgaAKgXNA9o6E63nRKLyM8+JnYDAC0yytUCDYK604nWhD4fmNgNoOvZg5CyNk3VaSPsop2Dq0XmZxYTu3GI3R76VD21Qhgn/DgiTUIaO946i5jYjUM8ypBcEoSdBr8xsTX3GcbEbhziOKFLBFc/POHkntBN8DOJid2YCl+Ci9K6wZ5kwCpigp9BTOzGAxw1lHeR462dD2Iin1lM7MYDHDmUV6YygKSJyLiEpkFHYxP/DGFiN05mWqED0kTSvbXWXFItKj9LmNgXERHE+9YwYpoNNMK+4LWNvqNt0O4BVCFGSNOM+Y2LxMS+gLjBAHf9GgSPFvl0Z9m7l0iCbFvxleKaTvjGXDBNDrqeiPwfEfm/IvLnIvIjXfmqiHxcRD7T3d94/NU1zgPpFehKawZBOF32SEngSyXbUXypD/d9M2aKaU69lcA3qOpXA28H3isi78QcYeYedY7Uz4jDnFT4vT3xKROavtD0jrd7ElVcrYRRJIwifrtEtkcwnqDRuvtZZJocdApsdw+z7qaYI8z844VmpaAZeFT2XV/qgVAPWxfXh9k1+yqR35sgVYN7fZ14bw1iRJuTFuKNy2DavPG+yyx7G/i4qj7gCAMc6wgjIi+KyIs15TlV2zgvkm8Pv6RM9myak2/zxGtgPwp/1HA9KVJHpGzQqkLL0oQ+w0wldlWNqvp24Bnga0Xkq6a9gDnCzAcpE8oVx+Ra6+32AHaCde45VaYaVV2nHa6/F3OEuVLEDOoloV7e93YzrhbTROOfFJHr3d994BuBT2GOMHOBhIDr9Q7dpN9Hi4xUBNTf12Uf4/pizD/TrLM/DfysiHjaH4dfUdWPicj/xhxhZhsR3LUVZGX5UHFcXaK62Sd1CSssy8xiME00/k9obZrvL7+LOcLMPJJlaL84tHEm9jOavnsg/ZSa5q80toNuQVAvpEGOekfsdetpAk1PSEGIOTZ0v+KY2BcEzTzVtZyUO9QBAuqgHrYbaNTm6VceE/tVpDvogveQZWjm0cyTssPLail0G2cO3A59zIFDL/ZDMP+Y2K8grihwN65DnhGfWKG+3iP2HOObnnhwq4NAzAWk3UTTDPXQjjlXC2Fnisw0xlxgYr+KZBm6NECLnGa5oF72NIWjGXD0hhnabbGxBynsb5VzHvxYrGe/IpjYFw2BFNrbQVIO6vSwqB3E4vDw3gXIN03584iJfQGJOdQrh4WtTh+Ys6eg1Mvt3H0XV0Jx72LqaZwvJvYFQl0bdVcPGnQvKn8sAip66AyMxPZ0nDqZLsuNMTOY2BeEFKBegmYAKddW9A60FyE/5vx545CxQ+K+qFWgGQqTxoFC7pxN5+cEE/uCoF5oBlAvHxC2KG7QUPTrI99TVYFUFYdTSDtoeoAKvhS4f2+9MbOY2K847e44R8z2A3DqgKCoV1w3T/c+EVxCRKmjJyXBiRKzRBupY7+H7+7UCakISJO3ySuNmca+oauMg2bJM14VYl/QbllNi4RfrhFRfIiIKHloeGppm+ASt3eW2BoX+JBguSKpEHcyZOdw2pqYQ7ma4/ue/nrvMlponAIT+1VGhOSFWNy3990rWd7g3P6QPvOJlXxCkMS67wPgXMLlCVVhXHpU/P5hmS7QFwtBkkOzYLZPM46J/SqjSpgk8i1HE4V65fDTIjAoKnIfGWYVrou7L+UlLEEVPTtlToyCzxNx2BALR9349hx86lJZiaD9HH9zFaqauL1jeeNnEBP7VSZBGEV6644yOuS+LIHOJVb7I57o7bSPuyTwq8WI1WLEZt3jVnONGB15UaN5Q1N7am0P1Eizf6gmLhWEJ1eRcYmrKtLExD5rmNivOgqStN0Yo7vWLvs4UZwkMkkE1wq0SZ6EECQSfCIL+8IVURqX7S3dpdDmr1PvwHW3LEOaBk1qPfwMYWI3ABiGkjf37pFJ5AvlDe5US/R8wxuXtmjS/ta67arg85ttMC7lUF5zNIWSbfs2x3gWcDdvQLyGbm0RNzZtHj8jTC32Li3Vi8AXVPV9IrIK/DLwFuBl4DtUde1xVNJ4/PR9zVNhg56r2WgG3GGJwjd7vf0uwSVuhXa4r0Fp+tIGAkMXuQse9e2PgdQ1bGxeaDuM4zlNdtnvA1468NgcYa4oy37CG4ot3lBs8XRv89DtyWKbol+TBomU0W2phVg44nJB6mdoOFXSYuOCmKpnF5FngH8E/AfgX3fF5ghzBfEoX5Kt8VS2QTwiKd2dbIXPXb/JF6JQpx56L7QuMkNh8kQPP4nkdxVpzAJq1ph2GP9jwA8BB9OUHnKEEbk/1tsiIh8EPgjQY/DoNTXOhh7IPJMgJSElR0yOpI4mORKOiOAk4Uhk92ndo4xc2Q7vQ6L2+3vsd51lXOP2x4tdxhyN0ebtM8CJYheR9wG3VfUPROTdp72Aqj4PPA+wIqv2jV8SLiphWyC1B1sqgSpLvOoS4yZjuy4ILjFw1ZHvfyLb4o1h43BhptQrSsqEMDriTf0e/skn0CaSNjfR0uy/LpNpevZ3Ad8mIt8K9IAVEfl5OkeYrlc3R5gZRyKEEbTxNkG9J+WOnaxHjI4qenqhpu+PPhQD8GQ4HGxzWaQZetQ70v3DAEB7OfRyqBtkMjGxXzInRlJU9cOq+oyqvgX4TuD3VPW7MEeYucDVCT9RQpkIE8WV4EtwleBKQaOg3dw8PWLieO1y2MVCiD0h9bI2UJf5Q/nqjcvlLOvsH8EcYWYaUcVvl7hJTeplQE7Tc/hakNQKc3zNw/DMF6LpC+NVR5gIkOMnGdlWjd+Y2Hn3GeFUYlfV36eNupsjzDygnaVy3a6V+0mGiuJzwVWd93o83KsfFYH3XV6qdCBvlezmquryzWuAVECD0BQOSPiJ4yH27sYFYzvoriBaVcj6FhI89Hvt3PnIF4KbOMZbBU3jeMVfowj7eaMzF7lejOn7mrVmwOfKNzBKOYNQcXNpxLpPbEdHrBx1bA/E+Bx8LcTS4ydKZkvuM4OJ/QqiZUm8cwfxHv/kE22Q7AgkgR8JkFFXjjtOCWF/fbyf1wxCRd/XbNR9tup2Z9xSVjIINf2wxCsqVFWgUlDviWU7RfAl5Num9FnCxH5VUT15fbtbeycBSWiaNkON94qIEpMwiRmhSYT7Dso06qiSJ0ZHiq2o9xJYahv9xxZaZwoTuwGA1ELcyIlOicOGoldTR89r20vc8wOuFROe7G+TVHh1tMJOlbM9Lhiv9SEKUrdzfWmg2FCyHSXbsRNvs4SJ3QBAGmlzzAmkPEEPYnRMUsYEyH3Xq6tjXGdsjwvKcfZA9llJ0s7VRwlXpXbUYMwEJvYFxDVKttP6vKW8Fap6SFnn9VY5JqMc55WQNXifmDSBO5MhZQxsjnqU44xUesRM3ecGE/sCEiaKr7Q7lupwlRB7UF0DnCITD6Uj5oq7lvA+MSpzJnWgaTzjtX7bo6sg1nPPDSb2RaQLoDkU1yiuEVJqg3VKd1gGgUh7WCbuRtUdsfHtHP3A0H0v0Ld70MaYSUzsxvE0QtzKiP7Av0liLxi3iyj4ieAn0u2/tzD8LGJiN45FEsh4irXyTuzZNviJtstuxsxhYjdOh4Kr5ZCzKwlc1QrdlxDK9vCNq21MP0uY2I1TIVEIO+0Oub2yBMWakm+1eep7r5e4skGahFjSipnBxL6giCo67Xm0A3oVBVeDr/bLJbWPfaX4UnFlgxsfOBdvgp8JTOwLip7ynLmrWtdWiV0CDAXXgB9rJ/YHBS2TCkZjtGnQ6ugMOMbFYWI3psJXkG11QbtO165Siq12+e7IffCjMc3tO6DJevcZwMRuHKZbK5d03zp6lK68XaOXbq1ekra37m9XJ6SMUNVo05jQZwgTu3EIUfBjIYwOD/PdAYFn20oodwUOKGTbDX6nxk1q5LW76GTSDt1N6DPDtHnjXwa2gAg0qvqcOcJcXVwthMnRz4m28/Mw1r1IuyRwVcLvVMhoQtraIk2O+QDj0jhNdoF/qKpvV9XnusfmCDOnTL0c9pCXiSrSQBglwk7EVbaTZtY5SyqR99M6wdDdf/uZa2M8dkR1+kj8CS/zVSK/Oya/O0LGx6egNmaDacWuwO+KyB90Di9wnyMMcKwjjIi8KCIv1lje8MtmV+jnstkltTZPUkckRmgiJNs1N6tMG6B7l6q+0lk8fVxEPjXtBcwRZjY57Tr7wz9Mke0xOhqhdUOqrJefRaYSu6q+0t3fFpFfB74Wc4QxdkmKjkbEu/cuuybGQzhxGC8iQxFZ3v0b+GbgzzBHmCuLBqXpQcxbtxfjajBNz/4U8OvSDvsC8N9U9bdF5JOYI8yVQwWaPsRCcTXkm4LYqPxKcKLYVfVzwFcfUW6OMHPIbjT+2IMw0vbsCqBiPfsVwrL4Lxi7gblzDdAZc4GJ3TAWBBO7YSwIJnbDWBBM7IaxIJjYDWNBMLEbxoJgySuMwxx0d7GTDFcKE7txiD13l3GXXLK57BoZ54WJfcGY5jy7K4Vs54IqZFwYNmdfMGzn3OJiYl8wzKFlcTGxGyfTBeyM+cbm7AvGIw3jbeR/JbCe3TgZ69WvBNazG1OxawghTevtZswfJnZjKvxEybdbobvauvp5ZKphvIhcF5GPisinROQlEfl6EVkVkY+LyGe6+xuPu7LG5eEaCJPWf91FE/s8Mu2c/ceB31bVr6BNUfUS5ggzt5xp+U3BjxPZZiSMo3m5zRHTZJddAf4B8FMAqlqp6jrmCDO3nGVjjUTINmvyOzv4jQlivfzcME3P/mXA68DPiMgfichPdimlzRHmqrNryxzZi8iLtoaOEhVJad8FJpnoZ51pxB6ArwF+QlXfAexwiiG7qj6vqs+p6nMZxSNW07gMXAPZltJbS2QjPbwEp4pMKri7Bq/fJY1Gl1ZPYzqmEfst4JaqvtA9/iit+F/rnGAwR5grSoJspGQ7iq+O6LnrhrS+QVzfQEsbtc06J4pdVb8IfF5Evrwreg/wF5gjjGHMFdOus/9L4BdEJAc+B3wP7Q+FOcIYxpwwrbHjHwPPHfGUOcIYxpxge+MNY0EwsS8Yj7Khxs7AXw1M7AvGo2yosew2VwMTu2EsCCb2BWN3B9yjvM+Yb+yI64LxqEPyI9+XBdz1a9A0pPHENtbMOCZ249EQQXs5ZDcgJdydNaKJfaaxYbzx6IhA8OAcOAvizTomdsNYEEzshrEg2JzdOB4HsRDUgWv0sO+bajuMN+YGE7txLMlDeU2QJGQ7SrF5IK2sCX3uMLEvKgL6kKCaCuAB13biybdl6qS9+e69CcS7Vvz3/wDY2vxMYWJfQGIhND0heUiZHHJ8UYHYV3CCNOAnbcaaWAjlNYdEqAd9fN3Dl4kwikidyFTx3h+6jm7vkHbMDnZWMLEvIDEXqpV2Ln7k84USC8VXgqsFGkg5VHn7q1Bqe5/tOHprDl8pkpRwsGdXbX9DRiPr4WcEE/tVRATJc8R7CEd/xSqAQAqgrhVzyhUNClGQ1L4mHfMfIgoxCrFoRwaxF3D9bP8FCXy/h+v30RjRqjLRXzInir1LR/XLB4q+DPh3wM915W8BXga+Q1XXzr+Kxmlx/T5u9QZkAc2O/4rVQTOE2FOavsKNCnFK2s6QsUODUi8f7+sYCwEcrlLUZaT8wFBBlTxzBOegqkn31iwp5SUzTQ66T6vq21X17cDfBUbAr2MmEbOL92i/QAc9yMKxkXMVSJkSc0WLRMgb8qKB0Ebd1UHqhvRH3npK04Om38YAmr7bu8W+Iw4y0qCH9gu4bz5vXDynHca/B/grVf1/IvJ+4N1d+c8Cvw/88PlVzTgPUj8j5R7NHDql3lwRSXry0lrtPK4WpAYNQrXs8ZVSbByTjda4VE4r9u8EfrH7+5BJhIgcaRJhXB4qQhxmVCtZG4ybYmlcRMl7DVo0J762ynIqzZAGKhVEIdsSwljx1dnrb5wvU4u9yyz7bcCHT3MBEfkg8EGAHoNTVc6YApE2EHewKM/a4XtwJN/26CqyJ/gUaMv27hW8EkIihPjQy6kKqkJKgviEOgUvgKJACmKZbWaU0/Ts3wL8oaq+1j1+TUSe7nr1Y00iVPV54HmAFVm1sd054waD9kz5AYHp8oD6ySExc3tBsxSgWnKkDOplobqmpBzq6xEZNPT6NU9d26LwD+/R6+RZG/WZVNlDX2fMHqcR+wfYH8LDvknERzCTiEtDegW6NAC/H2uNSwX1Umg3zLDbqwuxB7EnNANohkrKFDesKfo1K4MJT/a36fn6odebxIydKjexzyFTiV1EBsA3Af/8QPFHMJOImUGdQwuPekezlFMP/aE18qYQmoEQc2gGSuwnCIrzhwdbXo4efPV9zdCXjGPORtEndcP5ST+gjSDdzZhdpjWJGAE37yu7i5lEzA5eaFYKmoGnHnrGq4KGffHFHMob7dC9WYmElQpxCe8VOUbgB1nNdnhr8Tp1F9K/XSzxer7Ea6LUVaDZypDGltdmGdtBN6/sztFFwDvUOVLmiIUj5pByOdSzp7ydt6esC8ZlEefS3keIKEESjqOF33M1y37MJGUMQ0mvKeiHmhAiKQmN2x/Wq7P007OIiX0OkRBw11aQLENvrFDfHJJyx+RGoB62w/VqBdKBabV6fWCd3TllqVcyyGpu9nZ4duk2A3f0mtk1P8If80NwEA1KvSSoc/jSk1l6lJnBxD6HSAjI8hI66NFc71Ndz/YOt9RDoelDvaKk8HBxiig3emOe6G3zVLHF3y5eY3iM2KdFfbsFNwWht+Hs3PsMYWKfQ1QV6gaZVLgqx1WJdo86uKwNzIcd2V9PP9i7CqRaqKuAKozqnG1f0Pc1G3FIrQ//lxilgvW6z3ZdsF0VVFWgqQPUgqukrUNNexIuqh1+mSFM7HOIVhXxzl3Ee3xVU8TrpF4ALQila/e032nPq9dLQjM8/H7XeOoklFngi8BWP2ej6gNt1P1hjGPGZzefZH3cY3vUo1ovkNqRrzuybcGX0Lur+FIJOxHSQz/OuEBM7POIKlqWKCD9Hm7UR1IiDAIHO2Z1AuL2s8pA27Nn7dHUlBx1GRg7xTtlrRownmKdfassGJc5dRmQ0uEqwZet0P2kE3qZ8LUpfZYwsc85Op7gNraQSU6WeSTl+885IfmASrtlNvYE9SANhB1HqpQ6y5hER1UFxlVGcA8XaNV4Rut9ZOzxO458XXA1hHEr9FAq+WYkjCNhfYJb30LLqj3PblwqJvY5J41GpPEYyXO8CFIdOH8gAjrAxUBTCJPQij1MwJftSTXE01SORMaOFCcelpFa6K07whjCDvTvJSQq0v1GhHGid2eCjGvc2ibxtdtojDZ3nwFM7FcBVYgRygqZHFhvE8GXEV86VByu0b0hvQCawE/aTDMK4ISTTra6SDdch9AN2V3dpqWSBL6MSFkjZYVWtQl9hjCxXxE0RtLGJrJzIBuM9wRVJA4JvQCS0RQHDsw4IdtqI/axEOolTu7ZI/TuKWGkZCOl93qJqyJ+a4JsjyEldDSGGEllaUKfIUzsVwXVB9M+iRCKHB88UmdkPY/Eg+tw+0KsB22KqRNW3tr5eSf0bKshbJXIpIY792ju3ju35hjnj4l9QZCo+ElE0nE9rafp6VRiz0ZKthPxExuizxMm9gVB6ki2Nj52R5tfyoHi2Gyye6+rleJORdiuILXBOWM+MLFfcVQVYmpNH6ojltU6JxdXRXyZkPTwSburdS8Id+AicOyIwZgVTOxXGVV0axuJx6Sa8h6Wh5BnuCqSr1cPtYQCcE3C39lEt7YPX2c8PseKG48DE/sVJ41GrSvLEUiW4/MczTOkjoSNh+efA6Bu0LtrxM3Nc66p8bgxsS8ymqCukckpdrfVDak5OfOsMXtMm5bqB4B/RrtW86fA9wADzBFmrtGmId5dQza3pn9TSqTq4fvnjdlkGvunNwH/Cnibqo5F5Fdo88e/jdYR5iMi8iFaRxgziZgztK7Q2vatLwLT5hEJQF9EAm2P/grwflonGLr7bz/32hmGcW5M4/X2BeA/0maQfRXYUNXf5T5HGMAcYQxjhjlR7CJyg7YXfyvwJcBQRL5r2guIyAdF5EURebGmfPSaGoZxJqYZxn8j8Neq+rqq1sCvAX+PzhEG4CRHGFV9TlWfyyjOq96GYZySacT+N8A7RWQgIkKbK/4l9h1hwBxhDGPmOTEar6oviMhHgT8EGuCPaL3bljBHGMOYG0Qv8NTSiqzq14mZyBjG4+IF/QSbeu/IPc+Wwt8wFgQTu2EsCCZ2w1gQTOyGsSBcaIBORF4HdoA7F3bRx88TWHtmmavUnmna8qWq+uRRT1yo2AFE5EVVfe5CL/oYsfbMNlepPWdtiw3jDWNBMLEbxoJwGWJ//hKu+Tix9sw2V6k9Z2rLhc/ZDcO4HGwYbxgLwoWKXUTeKyKfFpHPdqms5gYRebOI/A8ReUlE/lxEvq8rXxWRj4vIZ7r7G5dd19MgIl5E/khEPtY9ntv2iMh1EfmoiHyq+56+fs7b8wPd/9qficgvikjvLO25MLGLiAf+M/AttPnrPiAib7uo658DDfCDqvqVwDuB7+3q/yHaXHzPAp/oHs8T30d7ZHmXeW7PjwO/rapfAXw1bbvmsj0Hcj8+p6pfBXja3I+P3h5VvZAb8PXA7xx4/GHgwxd1/cfQnt8Avgn4NPB0V/Y08OnLrtsp2vBM9w/zDcDHurK5bA+wAvw1XRzqQPm8tudNwOeBVdqj6B8Dvvks7bnIYfxu5Xe51ZXNHSLyFuAdwAvMdy6+HwN+CDjoCzWv7fky4HXgZ7ppyU+KyJA5bY8+htyPFyn2o87Yzt1SgIgsAb8KfL+qzq0tioi8D7itqn9w2XU5JwLwNcBPqOo7aLdlz8WQ/SjOmvvxKC5S7LeANx94/AxtSuq5QUQyWqH/gqr+Wlc8VS6+GeRdwLeJyMvALwHfICI/z/y25xZwS1Vf6B5/lFb889qeM+V+PIqLFPsngWdF5K0iktMGG37zAq9/Jrr8ez8FvKSqP3rgqbnMxaeqH1bVZ1T1LbTfxe+p6ncxv+35IvB5Efnyrug9wF8wp+3hceR+vOCgw7cCfwn8FfBvLzsIcsq6/33aacefAH/c3b4VuEkb5PpMd7962XV9hLa9m/0A3dy2B3g78GL3Hf134Mact+dHgE8Bfwb8V6A4S3tsB51hLAi2g84wFgQTu2EsCCZ2w1gQTOyGsSCY2A1jQTCxG8aCYGI3jAXBxG4YC8L/B1XDFDlLYJkwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3배 확대하여 stretching : 오래 걸림.. 컴퓨터야 미안!\n",
    "\n",
    "def stretch_img(img): #(2048,28,28,1) 반복문으로 넣음\n",
    "    #img=img.reshape(28,28).astype(int)\n",
    "    #img=Image.fromarray(img)\n",
    "    row = 84\n",
    "    col = 84\n",
    "    stretch_img = np.zeros((84,84))\n",
    "    high = 0\n",
    "    low = 8\n",
    "\n",
    "    for x in range(1 , row):\n",
    "        for y in range(1, col):\n",
    "            if high < img[x,y] :\n",
    "                high = img[x,y]\n",
    "            if low > img[x,y]:\n",
    "                low = img[x,y]\n",
    "    for x in range(1 , row):\n",
    "        for y in range(1, col):\n",
    "            stretch_img[x,y]=int((img[x,y]-low)*8/(high-low))\n",
    "   \n",
    "    return stretch_img#스트레칭된 이미지 출력\n",
    "\n",
    "\n",
    "x__train=[ stretch_img(img) for img in x_train]\n",
    "plt.imshow(x__train[0])\n",
    "#stretch_img(x_train[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    inputs = Input(shape = (84,84,1))\n",
    "\n",
    "    bn = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    conv = tf.keras.layers.Conv2D(64, kernel_size=5, strides=1, padding='same', activation='relu')(bn)\n",
    "    bn = tf.keras.layers.BatchNormalization()(conv)\n",
    "    conv = tf.keras.layers.Conv2D(64, kernel_size=2, strides=1, padding='same', activation='relu')(bn)\n",
    "    pool = tf.keras.layers.MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "    do=tf.keras.layers.Dropout(0.4)(pool)\n",
    "\n",
    "    bn = tf.keras.layers.BatchNormalization()(do)\n",
    "    conv = tf.keras.layers.Conv2D(128, kernel_size=5, strides=1, padding='same', activation='relu',kernel_regularizer = regularizers.l2(0.001))(bn)\n",
    "    bn = tf.keras.layers.BatchNormalization()(conv)\n",
    "    conv = tf.keras.layers.Conv2D(128, kernel_size=2, strides=1, padding='same', activation='relu',kernel_regularizer = regularizers.l2(0.001))(bn)\n",
    "    pool = tf.keras.layers.MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "    do=tf.keras.layers.Dropout(0.4)(pool)\n",
    "\n",
    "    bn = tf.keras.layers.BatchNormalization()(do)\n",
    "    conv = tf.keras.layers.Conv2D(256, kernel_size=2, strides=1, padding='same', activation='relu',kernel_regularizer = regularizers.l2(0.001))(bn)\n",
    "    bn = tf.keras.layers.BatchNormalization()(conv)\n",
    "    conv = tf.keras.layers.Conv2D(256, kernel_size=2, strides=1, padding='same', activation='relu',kernel_regularizer = regularizers.l2(0.001))(bn)\n",
    "    pool = tf.keras.layers.MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "    flatten = tf.keras.layers.Flatten()(pool)\n",
    "\n",
    "    bn = tf.keras.layers.BatchNormalization()(flatten)\n",
    "    dense = tf.keras.layers.Dense(1000, activation='relu')(bn)\n",
    "\n",
    "    bn = tf.keras.layers.BatchNormalization()(dense)\n",
    "    outputs = tf.keras.layers.Dense(10, activation='softmax')(bn)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 84, 84, 1)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 84, 84, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 84, 84, 64)        1664      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 84, 84, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 84, 84, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 42, 42, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 42, 42, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 42, 42, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 42, 42, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 42, 42, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 42, 42, 128)       65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 21, 21, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 21, 21, 256)       131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 21, 21, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 21, 21, 256)       262400    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 25600)             102400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              25601000  \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 26,402,406\n",
      "Trainable params: 26,347,924\n",
      "Non-trainable params: 54,482\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_cnn_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "  2/128 [..............................] - ETA: 5s - loss: 4.8340 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0279s vs `on_train_batch_end` time: 0.0539s). Check your callbacks.\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 4.1421 - accuracy: 0.2915\n",
      "Epoch 2/300\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 2.5855 - accuracy: 0.3994\n",
      "Epoch 3/300\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 2.2419 - accuracy: 0.4648\n",
      "Epoch 4/300\n",
      "128/128 [==============================] - 10s 82ms/step - loss: 2.1574 - accuracy: 0.5039\n",
      "Epoch 5/300\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 2.0015 - accuracy: 0.5562\n",
      "Epoch 6/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 1.8533 - accuracy: 0.5908\n",
      "Epoch 7/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.7508 - accuracy: 0.6089\n",
      "Epoch 8/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.7136 - accuracy: 0.6343\n",
      "Epoch 9/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.5772 - accuracy: 0.6602\n",
      "Epoch 10/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.4825 - accuracy: 0.6929\n",
      "Epoch 11/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 1.4236 - accuracy: 0.7007\n",
      "Epoch 12/300\n",
      "128/128 [==============================] - 11s 82ms/step - loss: 1.3829 - accuracy: 0.7144\n",
      "Epoch 13/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 1.3387 - accuracy: 0.7251\n",
      "Epoch 14/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 1.2760 - accuracy: 0.7539\n",
      "Epoch 15/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.1937 - accuracy: 0.7646\n",
      "Epoch 16/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.1401 - accuracy: 0.7783\n",
      "Epoch 17/300\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 1.0856 - accuracy: 0.7856\n",
      "Epoch 18/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 1.0521 - accuracy: 0.8022\n",
      "Epoch 19/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.0223 - accuracy: 0.8066\n",
      "Epoch 20/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.9704 - accuracy: 0.8281\n",
      "Epoch 21/300\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 0.9384 - accuracy: 0.8374\n",
      "Epoch 22/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.9096 - accuracy: 0.8384\n",
      "Epoch 23/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.8787 - accuracy: 0.8345\n",
      "Epoch 24/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.8552 - accuracy: 0.8486\n",
      "Epoch 25/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.8193 - accuracy: 0.8589\n",
      "Epoch 26/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.7885 - accuracy: 0.8638\n",
      "Epoch 27/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.7509 - accuracy: 0.8818\n",
      "Epoch 28/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.7523 - accuracy: 0.8672\n",
      "Epoch 29/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.7289 - accuracy: 0.8765\n",
      "Epoch 30/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.7173 - accuracy: 0.8823\n",
      "Epoch 31/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.6713 - accuracy: 0.8921\n",
      "Epoch 32/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.6857 - accuracy: 0.8843\n",
      "Epoch 33/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.6252 - accuracy: 0.9033\n",
      "Epoch 34/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.6258 - accuracy: 0.9038\n",
      "Epoch 35/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.5962 - accuracy: 0.9106\n",
      "Epoch 36/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.6040 - accuracy: 0.9062\n",
      "Epoch 37/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.5937 - accuracy: 0.9077\n",
      "Epoch 38/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.5640 - accuracy: 0.9121\n",
      "Epoch 39/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.5339 - accuracy: 0.9277\n",
      "Epoch 40/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.5506 - accuracy: 0.9209\n",
      "Epoch 41/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.5086 - accuracy: 0.9297\n",
      "Epoch 42/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.4993 - accuracy: 0.9331\n",
      "Epoch 43/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.4873 - accuracy: 0.9390\n",
      "Epoch 44/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.4825 - accuracy: 0.9370\n",
      "Epoch 45/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.4781 - accuracy: 0.9355\n",
      "Epoch 46/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.4668 - accuracy: 0.9370\n",
      "Epoch 47/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.4443 - accuracy: 0.9443\n",
      "Epoch 48/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.4283 - accuracy: 0.9517\n",
      "Epoch 49/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.4327 - accuracy: 0.9424\n",
      "Epoch 50/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.4318 - accuracy: 0.9492\n",
      "Epoch 51/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.4267 - accuracy: 0.9448\n",
      "Epoch 52/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.4034 - accuracy: 0.9570\n",
      "Epoch 53/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3890 - accuracy: 0.9634\n",
      "Epoch 54/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.4000 - accuracy: 0.9526\n",
      "Epoch 55/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.4057 - accuracy: 0.9526\n",
      "Epoch 56/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3906 - accuracy: 0.9556\n",
      "Epoch 57/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3689 - accuracy: 0.9683\n",
      "Epoch 58/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3743 - accuracy: 0.9644\n",
      "Epoch 59/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3781 - accuracy: 0.9595\n",
      "Epoch 60/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3600 - accuracy: 0.9600\n",
      "Epoch 61/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3682 - accuracy: 0.9614\n",
      "Epoch 62/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3486 - accuracy: 0.9648\n",
      "Epoch 63/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3451 - accuracy: 0.9727\n",
      "Epoch 64/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3411 - accuracy: 0.9702\n",
      "Epoch 65/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3240 - accuracy: 0.9746\n",
      "Epoch 66/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3320 - accuracy: 0.9702\n",
      "Epoch 67/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3276 - accuracy: 0.9722\n",
      "Epoch 68/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3240 - accuracy: 0.9727\n",
      "Epoch 69/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3468 - accuracy: 0.9634\n",
      "Epoch 70/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3184 - accuracy: 0.9766\n",
      "Epoch 71/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3281 - accuracy: 0.9663\n",
      "Epoch 72/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3146 - accuracy: 0.9771\n",
      "Epoch 73/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3204 - accuracy: 0.9717\n",
      "Epoch 74/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3101 - accuracy: 0.9751\n",
      "Epoch 75/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3107 - accuracy: 0.9756\n",
      "Epoch 76/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3056 - accuracy: 0.9756\n",
      "Epoch 77/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3011 - accuracy: 0.9761\n",
      "Epoch 78/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3134 - accuracy: 0.9746\n",
      "Epoch 79/300\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 0.2989 - accuracy: 0.9800\n",
      "Epoch 80/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2946 - accuracy: 0.9775\n",
      "Epoch 81/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2952 - accuracy: 0.9795\n",
      "Epoch 82/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.3088 - accuracy: 0.9702\n",
      "Epoch 83/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2860 - accuracy: 0.9819\n",
      "Epoch 84/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2909 - accuracy: 0.9839\n",
      "Epoch 85/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2892 - accuracy: 0.9814\n",
      "Epoch 86/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2940 - accuracy: 0.9785\n",
      "Epoch 87/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2984 - accuracy: 0.9785\n",
      "Epoch 88/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2967 - accuracy: 0.9731\n",
      "Epoch 89/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2936 - accuracy: 0.9746\n",
      "Epoch 90/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2973 - accuracy: 0.9761\n",
      "Epoch 91/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2819 - accuracy: 0.9824\n",
      "Epoch 92/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2785 - accuracy: 0.9819\n",
      "Epoch 93/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2812 - accuracy: 0.9800\n",
      "Epoch 94/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2872 - accuracy: 0.9780\n",
      "Epoch 95/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2812 - accuracy: 0.9814\n",
      "Epoch 96/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2865 - accuracy: 0.9805\n",
      "Epoch 97/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2835 - accuracy: 0.9814\n",
      "Epoch 98/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2876 - accuracy: 0.9805\n",
      "Epoch 99/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2877 - accuracy: 0.9814\n",
      "Epoch 100/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2803 - accuracy: 0.9805\n",
      "Epoch 101/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2876 - accuracy: 0.9785\n",
      "Epoch 102/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2746 - accuracy: 0.9834\n",
      "Epoch 103/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2800 - accuracy: 0.9834\n",
      "Epoch 104/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2770 - accuracy: 0.9824\n",
      "Epoch 105/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2799 - accuracy: 0.9810\n",
      "Epoch 106/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2715 - accuracy: 0.9849\n",
      "Epoch 107/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2694 - accuracy: 0.9814\n",
      "Epoch 108/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2725 - accuracy: 0.9819\n",
      "Epoch 109/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2781 - accuracy: 0.9790\n",
      "Epoch 110/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2765 - accuracy: 0.9795\n",
      "Epoch 111/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2757 - accuracy: 0.9824\n",
      "Epoch 112/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2739 - accuracy: 0.9839\n",
      "Epoch 113/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2781 - accuracy: 0.9814\n",
      "Epoch 114/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2793 - accuracy: 0.9805\n",
      "Epoch 115/300\n",
      "128/128 [==============================] - 10s 82ms/step - loss: 0.2802 - accuracy: 0.9780\n",
      "Epoch 116/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2665 - accuracy: 0.9863\n",
      "Epoch 117/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2745 - accuracy: 0.9819\n",
      "Epoch 118/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2748 - accuracy: 0.9805\n",
      "Epoch 119/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2706 - accuracy: 0.9814\n",
      "Epoch 120/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2633 - accuracy: 0.9863\n",
      "Epoch 121/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2825 - accuracy: 0.9766\n",
      "Epoch 122/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2721 - accuracy: 0.9795\n",
      "Epoch 123/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2835 - accuracy: 0.9746\n",
      "Epoch 124/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2769 - accuracy: 0.9810\n",
      "Epoch 125/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2670 - accuracy: 0.9834\n",
      "Epoch 126/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2661 - accuracy: 0.9863\n",
      "Epoch 127/300\n",
      "128/128 [==============================] - 10s 82ms/step - loss: 0.2799 - accuracy: 0.9805\n",
      "Epoch 128/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2716 - accuracy: 0.9824\n",
      "Epoch 129/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2780 - accuracy: 0.9785\n",
      "Epoch 130/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2653 - accuracy: 0.9858\n",
      "Epoch 131/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2726 - accuracy: 0.9839\n",
      "Epoch 132/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2669 - accuracy: 0.9849\n",
      "Epoch 133/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2760 - accuracy: 0.9795\n",
      "Epoch 134/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2799 - accuracy: 0.9814\n",
      "Epoch 135/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2707 - accuracy: 0.9824\n",
      "Epoch 136/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2900 - accuracy: 0.9785\n",
      "Epoch 137/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2778 - accuracy: 0.9810\n",
      "Epoch 138/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2737 - accuracy: 0.9805\n",
      "Epoch 139/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2603 - accuracy: 0.9883\n",
      "Epoch 140/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2604 - accuracy: 0.9873\n",
      "Epoch 141/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2794 - accuracy: 0.9800\n",
      "Epoch 142/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2709 - accuracy: 0.9819\n",
      "Epoch 143/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2766 - accuracy: 0.9790\n",
      "Epoch 144/300\n",
      "128/128 [==============================] - 10s 82ms/step - loss: 0.2667 - accuracy: 0.9834\n",
      "Epoch 145/300\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 0.2719 - accuracy: 0.9814\n",
      "Epoch 146/300\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 0.2683 - accuracy: 0.9810\n",
      "Epoch 147/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2695 - accuracy: 0.9854\n",
      "Epoch 148/300\n",
      "128/128 [==============================] - 11s 82ms/step - loss: 0.2697 - accuracy: 0.9834\n",
      "Epoch 149/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2616 - accuracy: 0.9854\n",
      "Epoch 150/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2605 - accuracy: 0.9868\n",
      "Epoch 151/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2644 - accuracy: 0.9854\n",
      "Epoch 152/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2748 - accuracy: 0.9810\n",
      "Epoch 153/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2729 - accuracy: 0.9800\n",
      "Epoch 154/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2721 - accuracy: 0.9814\n",
      "Epoch 155/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2765 - accuracy: 0.9761\n",
      "Epoch 156/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2740 - accuracy: 0.9834\n",
      "Epoch 157/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2811 - accuracy: 0.9810\n",
      "Epoch 158/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2648 - accuracy: 0.9858\n",
      "Epoch 159/300\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 0.2631 - accuracy: 0.9854\n",
      "Epoch 160/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2729 - accuracy: 0.9829\n",
      "Epoch 161/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2555 - accuracy: 0.9893\n",
      "Epoch 162/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2720 - accuracy: 0.9795\n",
      "Epoch 163/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2676 - accuracy: 0.9819\n",
      "Epoch 164/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2665 - accuracy: 0.9854\n",
      "Epoch 165/300\n",
      "128/128 [==============================] - 10s 82ms/step - loss: 0.2675 - accuracy: 0.9834\n",
      "Epoch 166/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2729 - accuracy: 0.9810\n",
      "Epoch 167/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2710 - accuracy: 0.9839\n",
      "Epoch 168/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2790 - accuracy: 0.9756\n",
      "Epoch 169/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2704 - accuracy: 0.9814\n",
      "Epoch 170/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2683 - accuracy: 0.9839\n",
      "Epoch 171/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2716 - accuracy: 0.9839\n",
      "Epoch 172/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2716 - accuracy: 0.9785\n",
      "Epoch 173/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2621 - accuracy: 0.9858\n",
      "Epoch 174/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2754 - accuracy: 0.9810\n",
      "Epoch 175/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2710 - accuracy: 0.9805\n",
      "Epoch 176/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2748 - accuracy: 0.9810\n",
      "Epoch 177/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2624 - accuracy: 0.9858\n",
      "Epoch 178/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2723 - accuracy: 0.9839\n",
      "Epoch 179/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2762 - accuracy: 0.9775\n",
      "Epoch 180/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2684 - accuracy: 0.9844\n",
      "Epoch 181/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2676 - accuracy: 0.9854\n",
      "Epoch 182/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2780 - accuracy: 0.9810\n",
      "Epoch 183/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2624 - accuracy: 0.9854\n",
      "Epoch 184/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2632 - accuracy: 0.9873\n",
      "Epoch 185/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2614 - accuracy: 0.9863\n",
      "Epoch 186/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2685 - accuracy: 0.9829\n",
      "Epoch 187/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2652 - accuracy: 0.9873\n",
      "Epoch 188/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2727 - accuracy: 0.9805\n",
      "Epoch 189/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2743 - accuracy: 0.9766\n",
      "Epoch 190/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2785 - accuracy: 0.9795\n",
      "Epoch 191/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2648 - accuracy: 0.9844\n",
      "Epoch 192/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2771 - accuracy: 0.9810\n",
      "Epoch 193/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2692 - accuracy: 0.9814\n",
      "Epoch 194/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2728 - accuracy: 0.9824\n",
      "Epoch 195/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2642 - accuracy: 0.9844\n",
      "Epoch 196/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2668 - accuracy: 0.9854\n",
      "Epoch 197/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2732 - accuracy: 0.9824\n",
      "Epoch 198/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2789 - accuracy: 0.9810\n",
      "Epoch 199/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2770 - accuracy: 0.9780\n",
      "Epoch 200/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2708 - accuracy: 0.9810\n",
      "Epoch 201/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2709 - accuracy: 0.9824\n",
      "Epoch 202/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2674 - accuracy: 0.9824\n",
      "Epoch 203/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2686 - accuracy: 0.9854\n",
      "Epoch 204/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2773 - accuracy: 0.9790\n",
      "Epoch 205/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2745 - accuracy: 0.9834\n",
      "Epoch 206/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2697 - accuracy: 0.9800\n",
      "Epoch 207/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2652 - accuracy: 0.9883\n",
      "Epoch 208/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2731 - accuracy: 0.9834\n",
      "Epoch 209/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2771 - accuracy: 0.9800\n",
      "Epoch 210/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2719 - accuracy: 0.9849\n",
      "Epoch 211/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2631 - accuracy: 0.9844\n",
      "Epoch 212/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2729 - accuracy: 0.9780\n",
      "Epoch 213/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2634 - accuracy: 0.9844\n",
      "Epoch 214/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2702 - accuracy: 0.9805\n",
      "Epoch 215/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2741 - accuracy: 0.9834\n",
      "Epoch 216/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2710 - accuracy: 0.9805\n",
      "Epoch 217/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2725 - accuracy: 0.9819\n",
      "Epoch 218/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2668 - accuracy: 0.9888\n",
      "Epoch 219/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2721 - accuracy: 0.9805\n",
      "Epoch 220/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2647 - accuracy: 0.9849\n",
      "Epoch 221/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2699 - accuracy: 0.9814\n",
      "Epoch 222/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2702 - accuracy: 0.9844\n",
      "Epoch 223/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2649 - accuracy: 0.9849\n",
      "Epoch 224/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2705 - accuracy: 0.9805\n",
      "Epoch 225/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2702 - accuracy: 0.9829\n",
      "Epoch 226/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2574 - accuracy: 0.9893\n",
      "Epoch 227/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2632 - accuracy: 0.9858\n",
      "Epoch 228/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2686 - accuracy: 0.9824\n",
      "Epoch 229/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2767 - accuracy: 0.9790\n",
      "Epoch 230/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2634 - accuracy: 0.9858\n",
      "Epoch 231/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2650 - accuracy: 0.9844\n",
      "Epoch 232/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2690 - accuracy: 0.9814\n",
      "Epoch 233/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2669 - accuracy: 0.9800\n",
      "Epoch 234/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2762 - accuracy: 0.9805\n",
      "Epoch 235/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2688 - accuracy: 0.9814\n",
      "Epoch 236/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2628 - accuracy: 0.9839\n",
      "Epoch 237/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2662 - accuracy: 0.9839\n",
      "Epoch 238/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2738 - accuracy: 0.9819\n",
      "Epoch 239/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2697 - accuracy: 0.9819\n",
      "Epoch 240/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2720 - accuracy: 0.9829\n",
      "Epoch 241/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2757 - accuracy: 0.9780\n",
      "Epoch 242/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2686 - accuracy: 0.9868\n",
      "Epoch 243/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2677 - accuracy: 0.9829\n",
      "Epoch 244/300\n",
      "128/128 [==============================] - 10s 82ms/step - loss: 0.2749 - accuracy: 0.98100s - loss: 0.2755 - accuracy: \n",
      "Epoch 245/300\n",
      "128/128 [==============================] - 11s 82ms/step - loss: 0.2775 - accuracy: 0.9800\n",
      "Epoch 246/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2699 - accuracy: 0.9839\n",
      "Epoch 247/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2712 - accuracy: 0.9829\n",
      "Epoch 248/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2753 - accuracy: 0.9800\n",
      "Epoch 249/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2682 - accuracy: 0.9868\n",
      "Epoch 250/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2691 - accuracy: 0.9819\n",
      "Epoch 251/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2674 - accuracy: 0.9824\n",
      "Epoch 252/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2816 - accuracy: 0.9756\n",
      "Epoch 253/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2710 - accuracy: 0.9854\n",
      "Epoch 254/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2697 - accuracy: 0.9839\n",
      "Epoch 255/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2741 - accuracy: 0.9839\n",
      "Epoch 256/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2562 - accuracy: 0.9893\n",
      "Epoch 257/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2653 - accuracy: 0.9824\n",
      "Epoch 258/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2652 - accuracy: 0.9873\n",
      "Epoch 259/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2664 - accuracy: 0.9824\n",
      "Epoch 260/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2739 - accuracy: 0.9814\n",
      "Epoch 261/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2725 - accuracy: 0.9819\n",
      "Epoch 262/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2696 - accuracy: 0.9844\n",
      "Epoch 263/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2726 - accuracy: 0.9810\n",
      "Epoch 264/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2759 - accuracy: 0.9819\n",
      "Epoch 265/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2777 - accuracy: 0.9775\n",
      "Epoch 266/300\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 0.2758 - accuracy: 0.9795\n",
      "Epoch 267/300\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.2652 - accuracy: 0.9888\n",
      "Epoch 268/300\n",
      "128/128 [==============================] - 10s 82ms/step - loss: 0.2784 - accuracy: 0.9814\n",
      "Epoch 269/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2592 - accuracy: 0.9863\n",
      "Epoch 270/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2713 - accuracy: 0.9863\n",
      "Epoch 271/300\n",
      "128/128 [==============================] - 11s 82ms/step - loss: 0.2623 - accuracy: 0.9854\n",
      "Epoch 272/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2695 - accuracy: 0.9814\n",
      "Epoch 273/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2594 - accuracy: 0.9863\n",
      "Epoch 274/300\n",
      "128/128 [==============================] - 10s 82ms/step - loss: 0.2713 - accuracy: 0.9805\n",
      "Epoch 275/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2708 - accuracy: 0.9839\n",
      "Epoch 276/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2659 - accuracy: 0.9854\n",
      "Epoch 277/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2755 - accuracy: 0.9795\n",
      "Epoch 278/300\n",
      "128/128 [==============================] - 10s 82ms/step - loss: 0.2647 - accuracy: 0.9834\n",
      "Epoch 279/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2693 - accuracy: 0.9824\n",
      "Epoch 280/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2692 - accuracy: 0.9824\n",
      "Epoch 281/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2754 - accuracy: 0.9810\n",
      "Epoch 282/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2670 - accuracy: 0.9844\n",
      "Epoch 283/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2619 - accuracy: 0.9854\n",
      "Epoch 284/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2679 - accuracy: 0.9810\n",
      "Epoch 285/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2639 - accuracy: 0.9854\n",
      "Epoch 286/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2577 - accuracy: 0.9883\n",
      "Epoch 287/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2722 - accuracy: 0.9824\n",
      "Epoch 288/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2697 - accuracy: 0.9849\n",
      "Epoch 289/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2731 - accuracy: 0.9814\n",
      "Epoch 290/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2772 - accuracy: 0.9824\n",
      "Epoch 291/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2731 - accuracy: 0.9829\n",
      "Epoch 292/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2611 - accuracy: 0.9863\n",
      "Epoch 293/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2695 - accuracy: 0.9785\n",
      "Epoch 294/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2650 - accuracy: 0.9863\n",
      "Epoch 295/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2681 - accuracy: 0.9834\n",
      "Epoch 296/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2636 - accuracy: 0.9834\n",
      "Epoch 297/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2696 - accuracy: 0.9805\n",
      "Epoch 298/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2716 - accuracy: 0.9824\n",
      "Epoch 299/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2630 - accuracy: 0.9839\n",
      "Epoch 300/300\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.2689 - accuracy: 0.9868\n"
     ]
    }
   ],
   "source": [
    "#1: stretch로 학습\n",
    "x__train=np.array(x__train).reshape(-1,84,84,1) \n",
    "x__train=x__train/8 #데이터 8비트라서 255보다는 8이 맞지않을까?\n",
    "\n",
    "y_ = train['digit']\n",
    "Y_train = np.zeros((len(y_), len(y_.unique())))\n",
    "for i, digit in enumerate(y_):\n",
    "    Y_train[i, digit] = 1\n",
    "    \n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  \n",
    "    zoom_range = 0.1, \n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1)\n",
    "\n",
    "model = create_cnn_model()\n",
    "epochs=300\n",
    "\n",
    "#model.fit(x_train, y_train, epochs=50)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "  datagen.flow(x__train, Y_train, batch_size=16),\n",
    "  epochs=epochs, \n",
    "  steps_per_epoch = x__train.shape[0]//16,\n",
    "  callbacks=[annealer], \n",
    "  verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/128 [..............................] - ETA: 5s - loss: 2.3573 - accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0299s vs `on_train_batch_end` time: 0.0568s). Check your callbacks.\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 2.9323 - accuracy: 0.3447\n",
      "Epoch 2/100\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 2.1626 - accuracy: 0.4058\n",
      "Epoch 3/100\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 1.8860 - accuracy: 0.4634\n",
      "Epoch 4/100\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 1.7404 - accuracy: 0.4858\n",
      "Epoch 5/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.6490 - accuracy: 0.5049\n",
      "Epoch 6/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.5691 - accuracy: 0.5229\n",
      "Epoch 7/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.5246 - accuracy: 0.5415\n",
      "Epoch 8/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.4820 - accuracy: 0.5483\n",
      "Epoch 9/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.4689 - accuracy: 0.5459\n",
      "Epoch 10/100\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 1.4149 - accuracy: 0.5640\n",
      "Epoch 11/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.3572 - accuracy: 0.5732\n",
      "Epoch 12/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.3582 - accuracy: 0.5786\n",
      "Epoch 13/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.2987 - accuracy: 0.5952\n",
      "Epoch 14/100\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 1.2549 - accuracy: 0.6055\n",
      "Epoch 15/100\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 1.2499 - accuracy: 0.6040\n",
      "Epoch 16/100\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 1.1984 - accuracy: 0.6270\n",
      "Epoch 17/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.2039 - accuracy: 0.6172\n",
      "Epoch 18/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.1632 - accuracy: 0.6450\n",
      "Epoch 19/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.1443 - accuracy: 0.6421\n",
      "Epoch 20/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.1236 - accuracy: 0.6543\n",
      "Epoch 21/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.0722 - accuracy: 0.6724\n",
      "Epoch 22/100\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 1.1008 - accuracy: 0.6611\n",
      "Epoch 23/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.0467 - accuracy: 0.6768\n",
      "Epoch 24/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.0324 - accuracy: 0.6870\n",
      "Epoch 25/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 1.0365 - accuracy: 0.6831\n",
      "Epoch 26/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.9720 - accuracy: 0.7061\n",
      "Epoch 27/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.9945 - accuracy: 0.7012\n",
      "Epoch 28/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.9454 - accuracy: 0.7070\n",
      "Epoch 29/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.9710 - accuracy: 0.7012\n",
      "Epoch 30/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.9349 - accuracy: 0.7109\n",
      "Epoch 31/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.9266 - accuracy: 0.7173\n",
      "Epoch 32/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.8915 - accuracy: 0.7300\n",
      "Epoch 33/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.8718 - accuracy: 0.7373\n",
      "Epoch 34/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.8499 - accuracy: 0.7378\n",
      "Epoch 35/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.8131 - accuracy: 0.7480\n",
      "Epoch 36/100\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.8161 - accuracy: 0.7622\n",
      "Epoch 37/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.7813 - accuracy: 0.7739\n",
      "Epoch 38/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.7932 - accuracy: 0.7627\n",
      "Epoch 39/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.7928 - accuracy: 0.7578\n",
      "Epoch 40/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.7970 - accuracy: 0.7563\n",
      "Epoch 41/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.7648 - accuracy: 0.7661\n",
      "Epoch 42/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.7461 - accuracy: 0.7749\n",
      "Epoch 43/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.7215 - accuracy: 0.7935\n",
      "Epoch 44/100\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.7082 - accuracy: 0.7979\n",
      "Epoch 45/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.7344 - accuracy: 0.7876\n",
      "Epoch 46/100\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.7230 - accuracy: 0.7935\n",
      "Epoch 47/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.6825 - accuracy: 0.7983\n",
      "Epoch 48/100\n",
      "128/128 [==============================] - 10s 81ms/step - loss: 0.6816 - accuracy: 0.8086\n",
      "Epoch 49/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.6546 - accuracy: 0.8159\n",
      "Epoch 50/100\n",
      "128/128 [==============================] - 10s 80ms/step - loss: 0.6819 - accuracy: 0.8052\n",
      "Epoch 51/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.6680 - accuracy: 0.8096\n",
      "Epoch 52/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.6395 - accuracy: 0.8188\n",
      "Epoch 53/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.6346 - accuracy: 0.8286\n",
      "Epoch 54/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.6688 - accuracy: 0.8071\n",
      "Epoch 55/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.6312 - accuracy: 0.8228\n",
      "Epoch 56/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.6100 - accuracy: 0.8291\n",
      "Epoch 57/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.6197 - accuracy: 0.8271\n",
      "Epoch 58/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5932 - accuracy: 0.8389\n",
      "Epoch 59/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.6257 - accuracy: 0.8164\n",
      "Epoch 60/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.6224 - accuracy: 0.8213\n",
      "Epoch 61/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5804 - accuracy: 0.8418\n",
      "Epoch 62/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5986 - accuracy: 0.8320\n",
      "Epoch 63/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5944 - accuracy: 0.8374\n",
      "Epoch 64/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5694 - accuracy: 0.8442\n",
      "Epoch 65/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.6012 - accuracy: 0.8218\n",
      "Epoch 66/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5736 - accuracy: 0.8433\n",
      "Epoch 67/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5650 - accuracy: 0.8516\n",
      "Epoch 68/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5649 - accuracy: 0.8442\n",
      "Epoch 69/100\n",
      "128/128 [==============================] - 10s 79ms/step - loss: 0.5259 - accuracy: 0.8706\n",
      "Epoch 70/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5789 - accuracy: 0.8389\n",
      "Epoch 71/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5471 - accuracy: 0.8486\n",
      "Epoch 72/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5550 - accuracy: 0.8496\n",
      "Epoch 73/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5618 - accuracy: 0.8408\n",
      "Epoch 74/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5568 - accuracy: 0.8398\n",
      "Epoch 75/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5396 - accuracy: 0.8491\n",
      "Epoch 76/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5608 - accuracy: 0.8472\n",
      "Epoch 77/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5486 - accuracy: 0.8521\n",
      "Epoch 78/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5465 - accuracy: 0.8491\n",
      "Epoch 79/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5407 - accuracy: 0.8574\n",
      "Epoch 80/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5393 - accuracy: 0.8569\n",
      "Epoch 81/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5200 - accuracy: 0.8584\n",
      "Epoch 82/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5092 - accuracy: 0.8691\n",
      "Epoch 83/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5279 - accuracy: 0.8652\n",
      "Epoch 84/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5427 - accuracy: 0.8481\n",
      "Epoch 85/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5387 - accuracy: 0.8501\n",
      "Epoch 86/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5218 - accuracy: 0.8540\n",
      "Epoch 87/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5232 - accuracy: 0.8647\n",
      "Epoch 88/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5224 - accuracy: 0.8589\n",
      "Epoch 89/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5196 - accuracy: 0.8604\n",
      "Epoch 90/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5040 - accuracy: 0.8662\n",
      "Epoch 91/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5153 - accuracy: 0.8638\n",
      "Epoch 92/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5125 - accuracy: 0.8623\n",
      "Epoch 93/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5256 - accuracy: 0.8530\n",
      "Epoch 94/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5210 - accuracy: 0.8608\n",
      "Epoch 95/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5052 - accuracy: 0.8667\n",
      "Epoch 96/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.4998 - accuracy: 0.8721\n",
      "Epoch 97/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.4988 - accuracy: 0.8823\n",
      "Epoch 98/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5124 - accuracy: 0.8564\n",
      "Epoch 99/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5091 - accuracy: 0.8711\n",
      "Epoch 100/100\n",
      "128/128 [==============================] - 10s 78ms/step - loss: 0.5447 - accuracy: 0.8418\n"
     ]
    }
   ],
   "source": [
    "#2:이진화하여 학습\n",
    "X__train = x__train*8\n",
    "X__train = [ cv2.threshold(a, 4,8,cv2.THRESH_BINARY)[1] for a in X__train ] #이진화\n",
    "X__train=np.array(X__train).reshape(-1,84,84,1) \n",
    "X__train=X__train/8 #데이터 8비트라서 255보다는 8이 맞지않을까?\n",
    "\n",
    "#threshold함수는 임계치와 이미지를 return하는것을 잊지말자..\n",
    "\n",
    "y = train['digit']\n",
    "y_train = np.zeros((len(y), len(y.unique())))\n",
    "for i, digit in enumerate(y):\n",
    "    y_train[i, digit] = 1\n",
    "    \n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "\n",
    "# 이미지 증식 사용\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  \n",
    "    zoom_range = 0.1, \n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1)\n",
    "\n",
    "#model = create_cnn_model() #엥 이거있으면 안되는데........?\n",
    "epochs=100\n",
    "\n",
    "#model.fit(x_train, y_train, epochs=50)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "  datagen.flow(X__train, y_train, batch_size=16),\n",
    "  epochs=epochs, \n",
    "  steps_per_epoch = X__train.shape[0]//16,\n",
    "  callbacks=[annealer], \n",
    "  verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2049</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2050</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2051</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2052</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2053</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2054</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2055</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2056</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2057</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2058</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2059</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2060</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2061</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2062</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2063</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2064</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2065</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2066</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2068</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2069</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2070</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2071</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2072</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2073</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2074</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2075</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2076</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2077</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2078</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2079</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2080</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2082</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2083</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2084</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2085</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2086</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2087</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2088</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2089</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2090</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2091</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2092</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2093</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2094</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2095</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2096</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2097</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2098</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  digit\n",
       "0   2049      2\n",
       "1   2050      2\n",
       "2   2051      5\n",
       "3   2052      2\n",
       "4   2053      8\n",
       "5   2054      8\n",
       "6   2055      8\n",
       "7   2056      8\n",
       "8   2057      8\n",
       "9   2058      8\n",
       "10  2059      8\n",
       "11  2060      8\n",
       "12  2061      2\n",
       "13  2062      8\n",
       "14  2063      8\n",
       "15  2064      8\n",
       "16  2065      3\n",
       "17  2066      0\n",
       "18  2067      2\n",
       "19  2068      8\n",
       "20  2069      3\n",
       "21  2070      8\n",
       "22  2071      8\n",
       "23  2072      2\n",
       "24  2073      5\n",
       "25  2074      2\n",
       "26  2075      2\n",
       "27  2076      8\n",
       "28  2077      2\n",
       "29  2078      2\n",
       "30  2079      2\n",
       "31  2080      2\n",
       "32  2081      0\n",
       "33  2082      5\n",
       "34  2083      2\n",
       "35  2084      8\n",
       "36  2085      2\n",
       "37  2086      2\n",
       "38  2087      2\n",
       "39  2088      8\n",
       "40  2089      5\n",
       "41  2090      2\n",
       "42  2091      8\n",
       "43  2092      8\n",
       "44  2093      2\n",
       "45  2094      3\n",
       "46  2095      8\n",
       "47  2096      5\n",
       "48  2097      5\n",
       "49  2098      8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = test.drop(['id', 'letter'], axis=1).values\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32')\n",
    "x_test = [cv2.resize(im,dsize=(84,84),interpolation=cv2.INTER_LINEAR) for im in x_test]\n",
    "#x_test1 = [cv2.threshold(a, 4,8,cv2.THRESH_BINARY)[1] for a in x_test1 ] #이진화\n",
    "#x_test=[ stretch_img(img) for img in x_test11] #stretch\n",
    "x_test = np.array(x_test).reshape(-1, 84, 84, 1)\n",
    "x_test = x_test/8\n",
    "\n",
    "\n",
    "#submission = pd.read_csv('data/submission.csv')\n",
    "submission['digit'] = np.argmax(model.predict(x_test), axis=1)\n",
    "\n",
    "submission.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
