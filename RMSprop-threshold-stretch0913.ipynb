{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "# from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, Input.\n",
    "\n",
    "# from tensorflow.keras.layers import Embedding\n",
    "# from keras import models\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers # L2규제\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # data augmentation\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler # callbacks 설정\n",
    "\n",
    "#from tqdm.notebook import tqdm # 모델학습 진행 시간 파악\n",
    "import random # random seed를 뽑을때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 크기 확대\n",
    "x_train = train.drop(['id', 'digit', 'letter'], axis=1).values\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') #resize에러 float32로 바꿔 해결함\n",
    "x_train = [cv2.resize(im,dsize=(84,84),interpolation=cv2.INTER_LINEAR) for im in x_train]\n",
    "#x_train = x_train/255 # data 정규화\n",
    "#img=x_train[0].reshape(28,28).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1af46f60d48>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3배 확대하여 stretching : 오래 걸림.. 컴퓨터야 미안!\n",
    "\n",
    "def stretch_img(img): #(2048,28,28,1) 반복문으로 넣음\n",
    "    #img=img.reshape(28,28).astype(int)\n",
    "    #img=Image.fromarray(img)\n",
    "    row = 84\n",
    "    col = 84\n",
    "    stretch_img = np.zeros((84,84))\n",
    "    high = 0\n",
    "    low = 8\n",
    "\n",
    "    for x in range(1 , row):\n",
    "        for y in range(1, col):\n",
    "            if high < img[x,y] :\n",
    "                high = img[x,y]\n",
    "            if low > img[x,y]:\n",
    "                low = img[x,y]\n",
    "    for x in range(1 , row):\n",
    "        for y in range(1, col):\n",
    "            stretch_img[x,y]=int((img[x,y]-low)*8/(high-low))\n",
    "   \n",
    "    return stretch_img#스트레칭된 이미지 출력\n",
    "\n",
    "\n",
    "x__train=[ stretch_img(img) for img in x_train]\n",
    "plt.imshow(x__train[0])\n",
    "#stretch_img(x_train[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    inputs = Input(shape = (84,84,1))\n",
    "\n",
    "    bn = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    conv = tf.keras.layers.Conv2D(64, kernel_size=5, strides=1, padding='same', activation='relu')(bn)\n",
    "    bn = tf.keras.layers.BatchNormalization()(conv)\n",
    "    conv = tf.keras.layers.Conv2D(64, kernel_size=2, strides=1, padding='same', activation='relu')(bn)\n",
    "    pool = tf.keras.layers.MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "    do=tf.keras.layers.Dropout(0.4)(pool)\n",
    "\n",
    "    bn = tf.keras.layers.BatchNormalization()(do)\n",
    "    conv = tf.keras.layers.Conv2D(128, kernel_size=5, strides=1, padding='same', activation='relu',kernel_regularizer = regularizers.l2(0.001))(bn)\n",
    "    bn = tf.keras.layers.BatchNormalization()(conv)\n",
    "    conv = tf.keras.layers.Conv2D(128, kernel_size=2, strides=1, padding='same', activation='relu',kernel_regularizer = regularizers.l2(0.001))(bn)\n",
    "    pool = tf.keras.layers.MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "    do=tf.keras.layers.Dropout(0.4)(pool)\n",
    "\n",
    "    bn = tf.keras.layers.BatchNormalization()(do)\n",
    "    conv = tf.keras.layers.Conv2D(256, kernel_size=2, strides=1, padding='same', activation='relu',kernel_regularizer = regularizers.l2(0.001))(bn)\n",
    "    bn = tf.keras.layers.BatchNormalization()(conv)\n",
    "    conv = tf.keras.layers.Conv2D(256, kernel_size=2, strides=1, padding='same', activation='relu',kernel_regularizer = regularizers.l2(0.001))(bn)\n",
    "    pool = tf.keras.layers.MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "    flatten = tf.keras.layers.Flatten()(pool)\n",
    "\n",
    "    bn = tf.keras.layers.BatchNormalization()(flatten)\n",
    "    dense = tf.keras.layers.Dense(1000, activation='relu')(bn)\n",
    "\n",
    "    bn = tf.keras.layers.BatchNormalization()(dense)\n",
    "    outputs = tf.keras.layers.Dense(10, activation='softmax')(bn)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "  2/128 [..............................] - ETA: 5s - loss: 6.9505 - accuracy: 0.1250    WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0299s vs `on_train_batch_end` time: 0.0649s). Check your callbacks.\n",
      "128/128 [==============================] - 12s 93ms/step - loss: 4.0388 - accuracy: 0.2656\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 12s 93ms/step - loss: 2.7607 - accuracy: 0.3667\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 12s 93ms/step - loss: 2.4462 - accuracy: 0.3877\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 12s 93ms/step - loss: 2.1598 - accuracy: 0.41890s - loss: 2.1\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 12s 93ms/step - loss: 2.0125 - accuracy: 0.4429\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.8293 - accuracy: 0.4653\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 12s 93ms/step - loss: 1.7596 - accuracy: 0.4751\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.6581 - accuracy: 0.4966\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.5844 - accuracy: 0.50881s - loss: 1.5879 - accuracy - ETA: 1s - loss:\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.5073 - accuracy: 0.5322\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.4812 - accuracy: 0.5283\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.4326 - accuracy: 0.5420\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.4005 - accuracy: 0.5547\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.3772 - accuracy: 0.5522\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.3230 - accuracy: 0.5698\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 1.2909 - accuracy: 0.6001\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 1.2889 - accuracy: 0.5864\n",
      "Epoch 18/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 1.2268 - accuracy: 0.5967\n",
      "Epoch 19/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.2399 - accuracy: 0.6094\n",
      "Epoch 20/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.2122 - accuracy: 0.6050\n",
      "Epoch 21/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.1602 - accuracy: 0.6235\n",
      "Epoch 22/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.1344 - accuracy: 0.6396\n",
      "Epoch 23/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.1339 - accuracy: 0.6343\n",
      "Epoch 24/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.0816 - accuracy: 0.6519\n",
      "Epoch 25/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.0613 - accuracy: 0.6475\n",
      "Epoch 26/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.0857 - accuracy: 0.6548\n",
      "Epoch 27/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.0313 - accuracy: 0.6631\n",
      "Epoch 28/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 1.0359 - accuracy: 0.6606\n",
      "Epoch 29/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.9989 - accuracy: 0.6797\n",
      "Epoch 30/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.9778 - accuracy: 0.6870\n",
      "Epoch 31/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.9874 - accuracy: 0.6895\n",
      "Epoch 32/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.9540 - accuracy: 0.6909\n",
      "Epoch 33/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.9096 - accuracy: 0.7100\n",
      "Epoch 34/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.9380 - accuracy: 0.7090\n",
      "Epoch 35/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.9136 - accuracy: 0.7080\n",
      "Epoch 36/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.8867 - accuracy: 0.7168\n",
      "Epoch 37/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.8747 - accuracy: 0.7188\n",
      "Epoch 38/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.8695 - accuracy: 0.7324\n",
      "Epoch 39/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.8540 - accuracy: 0.7280\n",
      "Epoch 40/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.8383 - accuracy: 0.7344\n",
      "Epoch 41/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.8437 - accuracy: 0.7153\n",
      "Epoch 42/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.8126 - accuracy: 0.7505\n",
      "Epoch 43/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.8095 - accuracy: 0.7593\n",
      "Epoch 44/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.7877 - accuracy: 0.7573\n",
      "Epoch 45/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.7653 - accuracy: 0.7622\n",
      "Epoch 46/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.7625 - accuracy: 0.7612\n",
      "Epoch 47/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.7482 - accuracy: 0.7720\n",
      "Epoch 48/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.7269 - accuracy: 0.7773\n",
      "Epoch 49/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.7587 - accuracy: 0.7729\n",
      "Epoch 50/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.7421 - accuracy: 0.7686\n",
      "Epoch 51/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.7363 - accuracy: 0.7749\n",
      "Epoch 52/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.7180 - accuracy: 0.7764\n",
      "Epoch 53/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.7046 - accuracy: 0.7793\n",
      "Epoch 54/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6964 - accuracy: 0.7930\n",
      "Epoch 55/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.7119 - accuracy: 0.7856\n",
      "Epoch 56/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.6850 - accuracy: 0.7998\n",
      "Epoch 57/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.6650 - accuracy: 0.7974\n",
      "Epoch 58/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.6490 - accuracy: 0.8145\n",
      "Epoch 59/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.6432 - accuracy: 0.8159\n",
      "Epoch 60/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6561 - accuracy: 0.8027\n",
      "Epoch 61/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.6565 - accuracy: 0.7920\n",
      "Epoch 62/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6348 - accuracy: 0.8066\n",
      "Epoch 63/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6413 - accuracy: 0.8164\n",
      "Epoch 64/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6050 - accuracy: 0.8193\n",
      "Epoch 65/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6437 - accuracy: 0.8091\n",
      "Epoch 66/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6112 - accuracy: 0.8179\n",
      "Epoch 67/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6152 - accuracy: 0.8184\n",
      "Epoch 68/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6350 - accuracy: 0.8159\n",
      "Epoch 69/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6245 - accuracy: 0.8140\n",
      "Epoch 70/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6126 - accuracy: 0.8154\n",
      "Epoch 71/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6236 - accuracy: 0.8179\n",
      "Epoch 72/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5826 - accuracy: 0.8369\n",
      "Epoch 73/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6047 - accuracy: 0.8188\n",
      "Epoch 74/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5996 - accuracy: 0.8257\n",
      "Epoch 75/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5701 - accuracy: 0.8452\n",
      "Epoch 76/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5876 - accuracy: 0.8301\n",
      "Epoch 77/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5756 - accuracy: 0.8262\n",
      "Epoch 78/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6051 - accuracy: 0.8218\n",
      "Epoch 79/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5906 - accuracy: 0.8335\n",
      "Epoch 80/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5747 - accuracy: 0.8330\n",
      "Epoch 81/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5891 - accuracy: 0.8320\n",
      "Epoch 82/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5870 - accuracy: 0.8267\n",
      "Epoch 83/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5800 - accuracy: 0.8306\n",
      "Epoch 84/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5798 - accuracy: 0.8306\n",
      "Epoch 85/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.6049 - accuracy: 0.8184\n",
      "Epoch 86/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5675 - accuracy: 0.8296\n",
      "Epoch 87/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5543 - accuracy: 0.8374\n",
      "Epoch 88/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5481 - accuracy: 0.8428\n",
      "Epoch 89/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5571 - accuracy: 0.8394\n",
      "Epoch 90/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5508 - accuracy: 0.8486\n",
      "Epoch 91/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5500 - accuracy: 0.8433\n",
      "Epoch 92/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5671 - accuracy: 0.8359\n",
      "Epoch 93/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5718 - accuracy: 0.8345\n",
      "Epoch 94/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5610 - accuracy: 0.8389\n",
      "Epoch 95/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5432 - accuracy: 0.8364\n",
      "Epoch 96/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5366 - accuracy: 0.8384\n",
      "Epoch 97/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5552 - accuracy: 0.8472\n",
      "Epoch 98/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5376 - accuracy: 0.8477\n",
      "Epoch 99/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5602 - accuracy: 0.83891s - los - ETA: 0s - loss: 0.5583 - ac\n",
      "Epoch 100/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5419 - accuracy: 0.8457\n",
      "Epoch 101/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5371 - accuracy: 0.8438\n",
      "Epoch 102/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5569 - accuracy: 0.8423\n",
      "Epoch 103/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5345 - accuracy: 0.8530\n",
      "Epoch 104/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5283 - accuracy: 0.8521\n",
      "Epoch 105/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5587 - accuracy: 0.8379\n",
      "Epoch 106/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5389 - accuracy: 0.8350\n",
      "Epoch 107/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5586 - accuracy: 0.8389\n",
      "Epoch 108/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5479 - accuracy: 0.8477\n",
      "Epoch 109/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5492 - accuracy: 0.8433\n",
      "Epoch 110/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5437 - accuracy: 0.8472\n",
      "Epoch 111/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5801 - accuracy: 0.8345\n",
      "Epoch 112/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5463 - accuracy: 0.8442\n",
      "Epoch 113/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.5554 - accuracy: 0.8394\n",
      "Epoch 114/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5211 - accuracy: 0.8521\n",
      "Epoch 115/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.5479 - accuracy: 0.8413\n",
      "Epoch 116/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.5349 - accuracy: 0.8481\n",
      "Epoch 117/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.5508 - accuracy: 0.8394\n",
      "Epoch 118/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.5422 - accuracy: 0.8418\n",
      "Epoch 119/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.5423 - accuracy: 0.8433\n",
      "Epoch 120/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.5331 - accuracy: 0.8462\n",
      "Epoch 121/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.5381 - accuracy: 0.8447\n",
      "Epoch 122/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5359 - accuracy: 0.8467\n",
      "Epoch 123/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5146 - accuracy: 0.8569\n",
      "Epoch 124/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5524 - accuracy: 0.8364\n",
      "Epoch 125/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5316 - accuracy: 0.8486\n",
      "Epoch 126/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5353 - accuracy: 0.8501\n",
      "Epoch 127/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5434 - accuracy: 0.8452\n",
      "Epoch 128/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5569 - accuracy: 0.8452\n",
      "Epoch 129/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5531 - accuracy: 0.8394\n",
      "Epoch 130/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5468 - accuracy: 0.8418\n",
      "Epoch 131/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5116 - accuracy: 0.8647\n",
      "Epoch 132/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5623 - accuracy: 0.8252\n",
      "Epoch 133/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5567 - accuracy: 0.8271\n",
      "Epoch 134/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5435 - accuracy: 0.8364\n",
      "Epoch 135/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5501 - accuracy: 0.8403\n",
      "Epoch 136/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5412 - accuracy: 0.8496\n",
      "Epoch 137/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5539 - accuracy: 0.8389\n",
      "Epoch 138/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.5358 - accuracy: 0.8462\n",
      "Epoch 139/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.5318 - accuracy: 0.8501\n",
      "Epoch 140/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5291 - accuracy: 0.8472\n",
      "Epoch 141/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.5486 - accuracy: 0.8394\n",
      "Epoch 142/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.5463 - accuracy: 0.8398\n",
      "Epoch 143/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.5536 - accuracy: 0.8350\n",
      "Epoch 144/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5325 - accuracy: 0.8501\n",
      "Epoch 145/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5514 - accuracy: 0.8433\n",
      "Epoch 146/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.5500 - accuracy: 0.8379\n",
      "Epoch 147/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5260 - accuracy: 0.8535\n",
      "Epoch 148/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.5306 - accuracy: 0.8433\n",
      "Epoch 149/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.5516 - accuracy: 0.8320\n",
      "Epoch 150/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.5079 - accuracy: 0.8608\n",
      "Epoch 151/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.5189 - accuracy: 0.8491\n",
      "Epoch 152/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5137 - accuracy: 0.8608\n",
      "Epoch 153/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5357 - accuracy: 0.8438\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5178 - accuracy: 0.8643\n",
      "Epoch 155/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5399 - accuracy: 0.8442\n",
      "Epoch 156/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5305 - accuracy: 0.8462\n",
      "Epoch 157/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5343 - accuracy: 0.8428\n",
      "Epoch 158/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5303 - accuracy: 0.8481\n",
      "Epoch 159/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5614 - accuracy: 0.8354\n",
      "Epoch 160/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5450 - accuracy: 0.8403\n",
      "Epoch 161/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5287 - accuracy: 0.8462\n",
      "Epoch 162/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5529 - accuracy: 0.8442\n",
      "Epoch 163/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5399 - accuracy: 0.8438\n",
      "Epoch 164/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5466 - accuracy: 0.8447\n",
      "Epoch 165/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5556 - accuracy: 0.8325\n",
      "Epoch 166/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5383 - accuracy: 0.8413\n",
      "Epoch 167/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5177 - accuracy: 0.8516\n",
      "Epoch 168/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5446 - accuracy: 0.8428\n",
      "Epoch 169/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5655 - accuracy: 0.8413\n",
      "Epoch 170/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5496 - accuracy: 0.8374\n",
      "Epoch 171/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5451 - accuracy: 0.84280s - loss: 0.5\n",
      "Epoch 172/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5362 - accuracy: 0.8364\n",
      "Epoch 173/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5250 - accuracy: 0.8545\n",
      "Epoch 174/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5567 - accuracy: 0.8438\n",
      "Epoch 175/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5313 - accuracy: 0.8535\n",
      "Epoch 176/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5438 - accuracy: 0.8379\n",
      "Epoch 177/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5262 - accuracy: 0.8530\n",
      "Epoch 178/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5463 - accuracy: 0.8394\n",
      "Epoch 179/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5288 - accuracy: 0.8477\n",
      "Epoch 180/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5328 - accuracy: 0.8511\n",
      "Epoch 181/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5410 - accuracy: 0.8433\n",
      "Epoch 182/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5502 - accuracy: 0.8379\n",
      "Epoch 183/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5292 - accuracy: 0.8457\n",
      "Epoch 184/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5308 - accuracy: 0.8477\n",
      "Epoch 185/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5486 - accuracy: 0.8457\n",
      "Epoch 186/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5239 - accuracy: 0.8467\n",
      "Epoch 187/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5383 - accuracy: 0.85160s - loss: 0.5383 - accuracy: 0.85\n",
      "Epoch 188/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5425 - accuracy: 0.8428\n",
      "Epoch 189/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5505 - accuracy: 0.8374\n",
      "Epoch 190/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5333 - accuracy: 0.8452\n",
      "Epoch 191/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5486 - accuracy: 0.8452\n",
      "Epoch 192/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5504 - accuracy: 0.8379\n",
      "Epoch 193/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5320 - accuracy: 0.8394\n",
      "Epoch 194/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5436 - accuracy: 0.8359\n",
      "Epoch 195/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5302 - accuracy: 0.8535\n",
      "Epoch 196/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5359 - accuracy: 0.8550\n",
      "Epoch 197/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5469 - accuracy: 0.8428\n",
      "Epoch 198/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5080 - accuracy: 0.8579\n",
      "Epoch 199/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5360 - accuracy: 0.8374\n",
      "Epoch 200/200\n",
      "128/128 [==============================] - 12s 94ms/step - loss: 0.5370 - accuracy: 0.83940s - loss: 0.5365 - ac\n"
     ]
    }
   ],
   "source": [
    "#1:이진화하여 학습\n",
    "X__train = [ cv2.threshold(a, 4,8,cv2.THRESH_BINARY)[1] for a in x__train ] #이진화\n",
    "X__train=np.array(X__train).reshape(-1,84,84,1) \n",
    "X__train=X__train/8 #데이터 8비트라서 255보다는 8이 맞지않을까?\n",
    "\n",
    "#threshold함수는 임계치와 이미지를 return하는것을 잊지말자..\n",
    "\n",
    "y = train['digit']\n",
    "y_train = np.zeros((len(y), len(y.unique())))\n",
    "for i, digit in enumerate(y):\n",
    "    y_train[i, digit] = 1\n",
    "    \n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "\n",
    "# 이미지 증식 사용\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  \n",
    "    zoom_range = 0.1, \n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1)\n",
    "\n",
    "model = create_cnn_model()\n",
    "epochs=200\n",
    "\n",
    "#model.fit(x_train, y_train, epochs=50)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "  datagen.flow(X__train, y_train, batch_size=16),\n",
    "  epochs=epochs, \n",
    "  steps_per_epoch = X__train.shape[0]//16,\n",
    "  callbacks=[annealer], \n",
    "  verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2049</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2051</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2052</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2053</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2054</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2055</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2056</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2057</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2058</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2059</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2060</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2062</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2063</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2064</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2065</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2066</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2067</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2068</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2069</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2070</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2071</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2072</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2073</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2074</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2075</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2076</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2077</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2078</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2079</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2080</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2082</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2083</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2084</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2085</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2086</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2087</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2088</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2089</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2090</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2091</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2092</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2093</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2094</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2095</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2096</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2097</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  digit\n",
       "0   2049      8\n",
       "1   2050      0\n",
       "2   2051      5\n",
       "3   2052      2\n",
       "4   2053      2\n",
       "5   2054      8\n",
       "6   2055      8\n",
       "7   2056      2\n",
       "8   2057      6\n",
       "9   2058      8\n",
       "10  2059      8\n",
       "11  2060      8\n",
       "12  2061      0\n",
       "13  2062      2\n",
       "14  2063      6\n",
       "15  2064      2\n",
       "16  2065      2\n",
       "17  2066      2\n",
       "18  2067      6\n",
       "19  2068      8\n",
       "20  2069      8\n",
       "21  2070      8\n",
       "22  2071      8\n",
       "23  2072      2\n",
       "24  2073      5\n",
       "25  2074      2\n",
       "26  2075      2\n",
       "27  2076      4\n",
       "28  2077      2\n",
       "29  2078      2\n",
       "30  2079      2\n",
       "31  2080      2\n",
       "32  2081      0\n",
       "33  2082      5\n",
       "34  2083      6\n",
       "35  2084      8\n",
       "36  2085      2\n",
       "37  2086      2\n",
       "38  2087      2\n",
       "39  2088      8\n",
       "40  2089      5\n",
       "41  2090      8\n",
       "42  2091      8\n",
       "43  2092      8\n",
       "44  2093      2\n",
       "45  2094      8\n",
       "46  2095      2\n",
       "47  2096      8\n",
       "48  2097      8\n",
       "49  2098      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = test.drop(['id', 'letter'], axis=1).values\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32')\n",
    "x_test = [cv2.resize(im,dsize=(84,84),interpolation=cv2.INTER_LINEAR) for im in x_test]\n",
    "x_test = [cv2.threshold(a, 4,8,cv2.THRESH_BINARY)[1] for a in x_test ] #이진화\n",
    "x_test = np.array(x_test).reshape(-1, 84, 84, 1)\n",
    "x_test = x_test/8\n",
    "\n",
    "\n",
    "\n",
    "submission['digit'] = np.argmax(model.predict(x_test), axis=1)\n",
    "\n",
    "submission.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('0913_test_thresholdOnly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "128/128 [==============================] - 15s 113ms/step - loss: 4.2959 - accuracy: 0.2710\n",
      "Epoch 2/200\n",
      "128/128 [==============================] - 15s 115ms/step - loss: 2.7436 - accuracy: 0.3745\n",
      "Epoch 3/200\n",
      "128/128 [==============================] - 13s 101ms/step - loss: 2.1863 - accuracy: 0.4805\n",
      "Epoch 4/200\n",
      "128/128 [==============================] - 14s 106ms/step - loss: 1.7999 - accuracy: 0.5547\n",
      "Epoch 5/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 1.5161 - accuracy: 0.6196\n",
      "Epoch 6/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 1.3894 - accuracy: 0.6299\n",
      "Epoch 7/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 1.2134 - accuracy: 0.6890\n",
      "Epoch 8/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 1.1215 - accuracy: 0.6982\n",
      "Epoch 9/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 1.0265 - accuracy: 0.7212\n",
      "Epoch 10/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.9049 - accuracy: 0.7646\n",
      "Epoch 11/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.8827 - accuracy: 0.7578\n",
      "Epoch 12/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.8513 - accuracy: 0.7690\n",
      "Epoch 13/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.7936 - accuracy: 0.8008\n",
      "Epoch 14/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.7380 - accuracy: 0.8032\n",
      "Epoch 15/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.6612 - accuracy: 0.8223\n",
      "Epoch 16/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.6446 - accuracy: 0.8359\n",
      "Epoch 17/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.6222 - accuracy: 0.8350\n",
      "Epoch 18/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.6229 - accuracy: 0.8394\n",
      "Epoch 19/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.6148 - accuracy: 0.8403\n",
      "Epoch 20/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.5682 - accuracy: 0.8472\n",
      "Epoch 21/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.5409 - accuracy: 0.8535\n",
      "Epoch 22/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.4893 - accuracy: 0.8701\n",
      "Epoch 23/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.4940 - accuracy: 0.8726\n",
      "Epoch 24/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.4707 - accuracy: 0.8813\n",
      "Epoch 25/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.4576 - accuracy: 0.8936\n",
      "Epoch 26/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.4379 - accuracy: 0.8892\n",
      "Epoch 27/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.4191 - accuracy: 0.8926\n",
      "Epoch 28/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.4050 - accuracy: 0.8975\n",
      "Epoch 29/200\n",
      "128/128 [==============================] - 12s 98ms/step - loss: 0.3937 - accuracy: 0.8989\n",
      "Epoch 30/200\n",
      "128/128 [==============================] - 13s 99ms/step - loss: 0.3531 - accuracy: 0.9077\n",
      "Epoch 31/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.3955 - accuracy: 0.9028\n",
      "Epoch 32/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.3693 - accuracy: 0.9087\n",
      "Epoch 33/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.3300 - accuracy: 0.9253\n",
      "Epoch 34/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.3339 - accuracy: 0.9268\n",
      "Epoch 35/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.3239 - accuracy: 0.9229\n",
      "Epoch 36/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2924 - accuracy: 0.9341\n",
      "Epoch 37/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.3286 - accuracy: 0.9204\n",
      "Epoch 38/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2932 - accuracy: 0.9341\n",
      "Epoch 39/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2670 - accuracy: 0.9487\n",
      "Epoch 40/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2682 - accuracy: 0.9399\n",
      "Epoch 41/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2562 - accuracy: 0.9463\n",
      "Epoch 42/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2694 - accuracy: 0.9419\n",
      "Epoch 43/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2487 - accuracy: 0.9448\n",
      "Epoch 44/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2435 - accuracy: 0.9546\n",
      "Epoch 45/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2465 - accuracy: 0.9458\n",
      "Epoch 46/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2466 - accuracy: 0.9502\n",
      "Epoch 47/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2257 - accuracy: 0.9531\n",
      "Epoch 48/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2292 - accuracy: 0.9556\n",
      "Epoch 49/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2294 - accuracy: 0.9536\n",
      "Epoch 50/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2385 - accuracy: 0.9512\n",
      "Epoch 51/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2163 - accuracy: 0.9585\n",
      "Epoch 52/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2204 - accuracy: 0.9536\n",
      "Epoch 53/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1979 - accuracy: 0.9639\n",
      "Epoch 54/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2007 - accuracy: 0.9619\n",
      "Epoch 55/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1901 - accuracy: 0.9644\n",
      "Epoch 56/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1939 - accuracy: 0.9609\n",
      "Epoch 57/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.2078 - accuracy: 0.9551\n",
      "Epoch 58/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1992 - accuracy: 0.9585\n",
      "Epoch 59/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1866 - accuracy: 0.9624\n",
      "Epoch 60/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1818 - accuracy: 0.9683\n",
      "Epoch 61/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1976 - accuracy: 0.9619\n",
      "Epoch 62/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1866 - accuracy: 0.9668\n",
      "Epoch 63/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1752 - accuracy: 0.9644\n",
      "Epoch 64/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1751 - accuracy: 0.9722\n",
      "Epoch 65/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1747 - accuracy: 0.9697\n",
      "Epoch 66/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1705 - accuracy: 0.9644\n",
      "Epoch 67/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1681 - accuracy: 0.9644\n",
      "Epoch 68/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1741 - accuracy: 0.9644\n",
      "Epoch 69/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1764 - accuracy: 0.9678\n",
      "Epoch 70/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1638 - accuracy: 0.9727\n",
      "Epoch 71/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1579 - accuracy: 0.9692\n",
      "Epoch 72/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1695 - accuracy: 0.9673\n",
      "Epoch 73/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1836 - accuracy: 0.9600\n",
      "Epoch 74/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1734 - accuracy: 0.9692\n",
      "Epoch 75/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1590 - accuracy: 0.9692\n",
      "Epoch 76/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1562 - accuracy: 0.9780\n",
      "Epoch 77/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1520 - accuracy: 0.9717\n",
      "Epoch 78/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1592 - accuracy: 0.9707\n",
      "Epoch 79/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1511 - accuracy: 0.9736\n",
      "Epoch 80/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1451 - accuracy: 0.9761\n",
      "Epoch 81/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1397 - accuracy: 0.9795\n",
      "Epoch 82/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1518 - accuracy: 0.9731\n",
      "Epoch 83/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1467 - accuracy: 0.9800\n",
      "Epoch 84/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1488 - accuracy: 0.9756\n",
      "Epoch 85/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1646 - accuracy: 0.97510s - loss: 0.1644 - \n",
      "Epoch 86/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1544 - accuracy: 0.9731\n",
      "Epoch 87/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1560 - accuracy: 0.97310s - loss: 0.1560 - accuracy: 0.97\n",
      "Epoch 88/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1620 - accuracy: 0.9717\n",
      "Epoch 89/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1566 - accuracy: 0.9751\n",
      "Epoch 90/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1568 - accuracy: 0.9702\n",
      "Epoch 91/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1426 - accuracy: 0.9790\n",
      "Epoch 92/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1459 - accuracy: 0.9766\n",
      "Epoch 93/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1480 - accuracy: 0.9722\n",
      "Epoch 94/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1467 - accuracy: 0.9761\n",
      "Epoch 95/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1474 - accuracy: 0.9761\n",
      "Epoch 96/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1446 - accuracy: 0.9756\n",
      "Epoch 97/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1567 - accuracy: 0.9692\n",
      "Epoch 98/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1455 - accuracy: 0.9731\n",
      "Epoch 99/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1483 - accuracy: 0.9727\n",
      "Epoch 100/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1336 - accuracy: 0.9814\n",
      "Epoch 101/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1392 - accuracy: 0.9756\n",
      "Epoch 102/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1460 - accuracy: 0.9722\n",
      "Epoch 103/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1435 - accuracy: 0.9722\n",
      "Epoch 104/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1282 - accuracy: 0.9810\n",
      "Epoch 105/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1464 - accuracy: 0.9736\n",
      "Epoch 106/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1513 - accuracy: 0.9741\n",
      "Epoch 107/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1414 - accuracy: 0.9736\n",
      "Epoch 108/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1370 - accuracy: 0.9751\n",
      "Epoch 109/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1439 - accuracy: 0.9741\n",
      "Epoch 110/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1331 - accuracy: 0.9819\n",
      "Epoch 111/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1463 - accuracy: 0.9751\n",
      "Epoch 112/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1409 - accuracy: 0.9780\n",
      "Epoch 113/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1396 - accuracy: 0.9756\n",
      "Epoch 114/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1418 - accuracy: 0.9741\n",
      "Epoch 115/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1415 - accuracy: 0.9785\n",
      "Epoch 116/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1245 - accuracy: 0.9868\n",
      "Epoch 117/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1349 - accuracy: 0.9805\n",
      "Epoch 118/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1344 - accuracy: 0.9810\n",
      "Epoch 119/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1451 - accuracy: 0.9761\n",
      "Epoch 120/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1349 - accuracy: 0.9780\n",
      "Epoch 121/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1285 - accuracy: 0.9800\n",
      "Epoch 122/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1458 - accuracy: 0.9741\n",
      "Epoch 123/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1414 - accuracy: 0.9766\n",
      "Epoch 124/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1473 - accuracy: 0.9756\n",
      "Epoch 125/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1336 - accuracy: 0.9839\n",
      "Epoch 126/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1332 - accuracy: 0.9805\n",
      "Epoch 127/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1391 - accuracy: 0.9775\n",
      "Epoch 128/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1381 - accuracy: 0.9785\n",
      "Epoch 129/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1375 - accuracy: 0.9727\n",
      "Epoch 130/200\n",
      "128/128 [==============================] - 13s 99ms/step - loss: 0.1396 - accuracy: 0.9756\n",
      "Epoch 131/200\n",
      "128/128 [==============================] - 13s 99ms/step - loss: 0.1314 - accuracy: 0.9766\n",
      "Epoch 132/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1479 - accuracy: 0.9761\n",
      "Epoch 133/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1439 - accuracy: 0.9746\n",
      "Epoch 134/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1403 - accuracy: 0.9761\n",
      "Epoch 135/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1337 - accuracy: 0.9810\n",
      "Epoch 136/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1265 - accuracy: 0.9819\n",
      "Epoch 137/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1426 - accuracy: 0.9746\n",
      "Epoch 138/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1358 - accuracy: 0.9756\n",
      "Epoch 139/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1502 - accuracy: 0.9731\n",
      "Epoch 140/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1345 - accuracy: 0.9775\n",
      "Epoch 141/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1314 - accuracy: 0.9810\n",
      "Epoch 142/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1404 - accuracy: 0.9766\n",
      "Epoch 143/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1399 - accuracy: 0.9780\n",
      "Epoch 144/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1369 - accuracy: 0.9780\n",
      "Epoch 145/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1295 - accuracy: 0.9810\n",
      "Epoch 146/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1333 - accuracy: 0.9795\n",
      "Epoch 147/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1515 - accuracy: 0.9707\n",
      "Epoch 148/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1377 - accuracy: 0.97850s - loss: 0.1391 - accuracy\n",
      "Epoch 149/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1443 - accuracy: 0.9727\n",
      "Epoch 150/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1367 - accuracy: 0.9771\n",
      "Epoch 151/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1355 - accuracy: 0.9795\n",
      "Epoch 152/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1370 - accuracy: 0.9771\n",
      "Epoch 153/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1261 - accuracy: 0.9824\n",
      "Epoch 154/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1527 - accuracy: 0.9722\n",
      "Epoch 155/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1268 - accuracy: 0.9829\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1382 - accuracy: 0.9795\n",
      "Epoch 157/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1475 - accuracy: 0.9766\n",
      "Epoch 158/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1345 - accuracy: 0.9819\n",
      "Epoch 159/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1404 - accuracy: 0.9775\n",
      "Epoch 160/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1346 - accuracy: 0.9761\n",
      "Epoch 161/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1504 - accuracy: 0.9707\n",
      "Epoch 162/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1359 - accuracy: 0.9800\n",
      "Epoch 163/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1380 - accuracy: 0.9775\n",
      "Epoch 164/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1310 - accuracy: 0.9790\n",
      "Epoch 165/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1551 - accuracy: 0.9702\n",
      "Epoch 166/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1367 - accuracy: 0.9775\n",
      "Epoch 167/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1350 - accuracy: 0.9766\n",
      "Epoch 168/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1346 - accuracy: 0.9805\n",
      "Epoch 169/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1320 - accuracy: 0.9814\n",
      "Epoch 170/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1357 - accuracy: 0.9771\n",
      "Epoch 171/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1361 - accuracy: 0.9751\n",
      "Epoch 172/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1438 - accuracy: 0.9756\n",
      "Epoch 173/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1331 - accuracy: 0.9785\n",
      "Epoch 174/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1332 - accuracy: 0.9790\n",
      "Epoch 175/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1397 - accuracy: 0.9771\n",
      "Epoch 176/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1406 - accuracy: 0.9775\n",
      "Epoch 177/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1411 - accuracy: 0.9780\n",
      "Epoch 178/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1398 - accuracy: 0.9756\n",
      "Epoch 179/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1314 - accuracy: 0.9800\n",
      "Epoch 180/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1266 - accuracy: 0.9829\n",
      "Epoch 181/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1338 - accuracy: 0.9790\n",
      "Epoch 182/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1353 - accuracy: 0.9805\n",
      "Epoch 183/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1277 - accuracy: 0.9819\n",
      "Epoch 184/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1371 - accuracy: 0.9829\n",
      "Epoch 185/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1263 - accuracy: 0.9800\n",
      "Epoch 186/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1354 - accuracy: 0.9810\n",
      "Epoch 187/200\n",
      "128/128 [==============================] - 12s 95ms/step - loss: 0.1373 - accuracy: 0.9805\n",
      "Epoch 188/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1403 - accuracy: 0.9761\n",
      "Epoch 189/200\n",
      "128/128 [==============================] - 12s 97ms/step - loss: 0.1312 - accuracy: 0.9819\n",
      "Epoch 190/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1262 - accuracy: 0.9814\n",
      "Epoch 191/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1415 - accuracy: 0.9736\n",
      "Epoch 192/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1433 - accuracy: 0.97511s\n",
      "Epoch 193/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1358 - accuracy: 0.9800\n",
      "Epoch 194/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1460 - accuracy: 0.9712\n",
      "Epoch 195/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1326 - accuracy: 0.9805\n",
      "Epoch 196/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1275 - accuracy: 0.9819\n",
      "Epoch 197/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1303 - accuracy: 0.9756\n",
      "Epoch 198/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1306 - accuracy: 0.9814\n",
      "Epoch 199/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1503 - accuracy: 0.9712\n",
      "Epoch 200/200\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 0.1388 - accuracy: 0.9756\n"
     ]
    }
   ],
   "source": [
    "#2: stretch로 학습\n",
    "x__train=np.array(x__train).reshape(-1,84,84,1) \n",
    "x__train=x__train/8 #데이터 8비트라서 255보다는 8이 맞지않을까?\n",
    "\n",
    "y_ = train['digit']\n",
    "Y_train = np.zeros((len(y_), len(y_.unique())))\n",
    "for i, digit in enumerate(y_):\n",
    "    Y_train[i, digit] = 1\n",
    "    \n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  \n",
    "    zoom_range = 0.1, \n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1)\n",
    "\n",
    "model = create_cnn_model() #이걸 그대로 쓰면 모델 초기화 되는것임..^^\n",
    "epochs=200\n",
    "\n",
    "#model.fit(x_train, y_train, epochs=50)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "  datagen.flow(x__train, Y_train, batch_size=16),\n",
    "  epochs=epochs, \n",
    "  steps_per_epoch = x__train.shape[0]//16,\n",
    "  callbacks=[annealer], \n",
    "  verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2049</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2050</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2051</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2053</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2054</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2055</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2056</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2057</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2058</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2059</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2060</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2061</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2062</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2064</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2066</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2068</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2069</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2070</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2071</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2072</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2073</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2074</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2076</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2077</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2078</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2079</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2080</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2082</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2084</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2086</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2087</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2088</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2089</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2090</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2091</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2092</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2093</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2095</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2096</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2097</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2098</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  digit\n",
       "0   2049      6\n",
       "1   2050      3\n",
       "2   2051      8\n",
       "3   2052      0\n",
       "4   2053      3\n",
       "5   2054      7\n",
       "6   2055      8\n",
       "7   2056      3\n",
       "8   2057      4\n",
       "9   2058      4\n",
       "10  2059      1\n",
       "11  2060      5\n",
       "12  2061      7\n",
       "13  2062      5\n",
       "14  2063      1\n",
       "15  2064      8\n",
       "16  2065      1\n",
       "17  2066      6\n",
       "18  2067      1\n",
       "19  2068      4\n",
       "20  2069      5\n",
       "21  2070      9\n",
       "22  2071      7\n",
       "23  2072      2\n",
       "24  2073      3\n",
       "25  2074      7\n",
       "26  2075      0\n",
       "27  2076      1\n",
       "28  2077      9\n",
       "29  2078      2\n",
       "30  2079      8\n",
       "31  2080      8\n",
       "32  2081      0\n",
       "33  2082      6\n",
       "34  2083      0\n",
       "35  2084      4\n",
       "36  2085      1\n",
       "37  2086      5\n",
       "38  2087      4\n",
       "39  2088      4\n",
       "40  2089      2\n",
       "41  2090      1\n",
       "42  2091      8\n",
       "43  2092      9\n",
       "44  2093      3\n",
       "45  2094      0\n",
       "46  2095      7\n",
       "47  2096      6\n",
       "48  2097      2\n",
       "49  2098      8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test11 = test.drop(['id', 'letter'], axis=1).values\n",
    "x_test11 = x_test11.reshape(-1, 28, 28, 1).astype('float32')\n",
    "x_test11 = [cv2.resize(im,dsize=(84,84),interpolation=cv2.INTER_LINEAR) for im in x_test11]\n",
    "#x_test1 = [cv2.threshold(a, 4,8,cv2.THRESH_BINARY)[1] for a in x_test1 ] #이진화\n",
    "x_test11=[ stretch_img(img) for img in x_test11] #stretch\n",
    "x_test11 = np.array(x_test11).reshape(-1, 84, 84, 1)\n",
    "x_test11 = x_test11/8\n",
    "\n",
    "\n",
    "submission1 = pd.read_csv('data/submission.csv')\n",
    "submission1['digit'] = np.argmax(model.predict(x_test11), axis=1)\n",
    "\n",
    "submission1.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1.to_csv('0913_test_thresholdNstretching.csv', index=False) #87점..!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
